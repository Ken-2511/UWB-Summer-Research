{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a794668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# Nested configuration structure\n",
    "@dataclass(frozen=True)\n",
    "class SignalConfig:\n",
    "    \"\"\"Signal parameters configuration\"\"\"\n",
    "    fc: float\n",
    "    fsymbol: float\n",
    "    fs: float\n",
    "    up_fs: float\n",
    "    fs_num: int\n",
    "    up_fs_num: int\n",
    "    \n",
    "    # Calculated properties\n",
    "    symbol_duration: float = field(init=False)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        object.__setattr__(self, 'symbol_duration', 1 / self.fsymbol)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class CSVConfig:\n",
    "    \"\"\"CSV file paths configuration\"\"\"\n",
    "    original: str\n",
    "    t0: str\n",
    "    pn_t0: str\n",
    "    fs: str\n",
    "    up_fs: str\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class AWGNConfig:\n",
    "    \"\"\"AWGN parameters configuration\"\"\"\n",
    "    snr_db: float\n",
    "    signal_power: float\n",
    "\n",
    "    # Calculated properties\n",
    "    snr_linear: float = field(init=False)\n",
    "    noise_power: float = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        object.__setattr__(self, 'snr_linear', 10 ** (self.snr_db / 10))\n",
    "        object.__setattr__(self, 'noise_power', self.signal_power / self.snr_linear)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PhaseNoiseConfig:\n",
    "    \"\"\"Phase noise parameters configuration\"\"\"\n",
    "    std_rad: float  # Standard deviation in radians\n",
    "    \n",
    "    # Calculated properties\n",
    "    std_degree: float = field(init=False)\n",
    "    std_time: float = field(init=False)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        object.__setattr__(self, 'std_degree', self.std_rad * 180 / np.pi)\n",
    "        object.__setattr__(self, 'std_time', self.std_rad / (2 * np.pi * config.signal.fc))\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Config:\n",
    "    \"\"\"Main configuration class\"\"\"\n",
    "    signal: SignalConfig\n",
    "    csv: CSVConfig\n",
    "    awgn: AWGNConfig\n",
    "    pn: PhaseNoiseConfig\n",
    "\n",
    "\n",
    "# Create configuration instance\n",
    "config = Config(\n",
    "    signal=SignalConfig(\n",
    "        fc=4e9,\n",
    "        fsymbol=500e6,\n",
    "        fs=32e9,\n",
    "        up_fs=1024e9,\n",
    "        fs_num=32,\n",
    "        up_fs_num=1024\n",
    "    ),\n",
    "    csv=CSVConfig(\n",
    "        original='../csv/8PSK2PPM_500MBps.csv',\n",
    "        t0='../csv/t0.csv',\n",
    "        pn_t0='../csv/pn_t0.csv',\n",
    "        fs='../csv/fs.csv',\n",
    "        up_fs='../csv/up_fs.csv'\n",
    "    ),\n",
    "        awgn=AWGNConfig(\n",
    "        snr_db=15,\n",
    "        signal_power=0.5\n",
    "    ),\n",
    "        pn=PhaseNoiseConfig(\n",
    "        std_rad=0.27\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04af2def",
   "metadata": {},
   "source": [
    "## Pre-configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d271f65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-configure the original file:\n",
    "# 1. change the header to be time,data\n",
    "# 2. make sure the time starts from 0\n",
    "# 3. make the 99% of the data to be 0.99\n",
    "\n",
    "# Read CSV file\n",
    "df = pd.read_csv(config.csv.original)\n",
    "\n",
    "# Get column names and rename to time and data\n",
    "columns = df.columns.tolist()\n",
    "df.columns = ['time', 'data']\n",
    "\n",
    "# Ensure time starts from 0\n",
    "if len(df) > 0:\n",
    "    time_start = df['time'].iloc[0]\n",
    "    df['time'] = df['time'] - time_start\n",
    "\n",
    "# Scale, make the 99% of the data to be 0.99\n",
    "factor = 0.99 / df['data'].quantile(0.99)\n",
    "df['data'] = df['data'] * factor\n",
    "\n",
    "# Save processed file\n",
    "df.to_csv(config.csv.t0, index=False)\n",
    "\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf573aef",
   "metadata": {},
   "source": [
    "## Add phase noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c883ffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data = pd.read_csv(config.csv.t0)\n",
    "\n",
    "# generate phase noise\n",
    "np.random.seed(42)\n",
    "phase_noise = np.random.normal(0, config.pn.std_rad, len(data))\n",
    "\n",
    "# Calculate time jitter noise\n",
    "# Relationship between time jitter and phase noise: Δt = Δφ / (2π * f_carrier)\n",
    "time_jitter = phase_noise / (2 * np.pi * config.signal.fc)\n",
    "\n",
    "# Create output dataframe, keep only time and data columns\n",
    "output_data = pd.DataFrame()\n",
    "\n",
    "# Add time column\n",
    "output_data['time'] = data['time']\n",
    "\n",
    "# Apply time jitter noise to the original signal\n",
    "# Get original time axis\n",
    "t_original = np.array(data['time'].values)\n",
    "t_jittered = t_original + time_jitter\n",
    "data_original = np.array(data['data'].values)\n",
    "\n",
    "# Use interpolation to get signal values at jittered time points\n",
    "# from scipy.interpolate import interp1d\n",
    "\n",
    "# Create interpolation function\n",
    "# Use linear interpolation, extrapolate for boundaries\n",
    "interp_func = sp.interpolate.interp1d(t_original, data_original,\n",
    "                        kind='linear',\n",
    "                        bounds_error=False,\n",
    "                        fill_value=0)\n",
    "\n",
    "# Sample at jittered time points\n",
    "noisy_signal = interp_func(t_jittered)\n",
    "\n",
    "output_data['data'] = noisy_signal\n",
    "\n",
    "# save to csv\n",
    "output_data.to_csv(config.csv.pn_t0, index=False)\n",
    "\n",
    "plot_data = output_data.head(3000)\n",
    "\n",
    "# Read and plot the first 1000 points\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(plot_data['time'] * 1e9, plot_data['data'], label='data')\n",
    "plt.xlabel('Time (ns)')\n",
    "plt.ylabel('Signal value')\n",
    "plt.title('First 1000 points of phase noise/time jitter signal')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b9d830",
   "metadata": {},
   "source": [
    "## Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74541430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the target resampling frequency and interval\n",
    "target_frequency_hz = config.signal.fs\n",
    "resampling_interval_s = 1 / target_frequency_hz\n",
    "\n",
    "# Read the CSV file, skipping the original header row to replace it later\n",
    "df = pd.read_csv(config.csv.pn_t0, header=0)\n",
    "\n",
    "# Rename columns for clarity based on the original header structure\n",
    "# Assuming the first column is time-like and second is data-like\n",
    "df.columns = ['original_time', 'original_data']\n",
    "\n",
    "# Convert columns to numeric, coercing errors if any\n",
    "df['original_time'] = pd.to_numeric(df['original_time'], errors='coerce')\n",
    "df['original_data'] = pd.to_numeric(df['original_data'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values that might have resulted from coercion\n",
    "df.dropna(subset=['original_time', 'original_data'], inplace=True)\n",
    "\n",
    "# Prepare for resampling\n",
    "# The new time axis will start from 0 (because df['original_time'] now starts from 0)\n",
    "# and go up to the maximum duration of the adjusted time\n",
    "start_resample_time = 0\n",
    "end_resample_time = df['original_time'].max()\n",
    "\n",
    "new_time_axis = np.arange(start_resample_time, end_resample_time, resampling_interval_s)\n",
    "\n",
    "# Perform linear interpolation\n",
    "# np.interp needs the original x-values (df['time']) to be sorted\n",
    "# Assert that the data is already sorted by time\n",
    "assert df['original_time'].is_monotonic_increasing, \"Data must be sorted by time\"\n",
    "resampled_data_values = np.interp(new_time_axis, df['original_time'], df['original_data'])\n",
    "\n",
    "# Create a new DataFrame for the resampled data\n",
    "df_resampled = pd.DataFrame({'time': new_time_axis, 'data': resampled_data_values})\n",
    "\n",
    "# Display the resampled data information without saving to file\n",
    "print(f\"Data resampled to {config.signal.fs_num} GHz.\")\n",
    "print(df_resampled.head())\n",
    "\n",
    "# Display comparison of original and resampled signals for the first 30ns\n",
    "time_limit = 1e-8  # 10 ns\n",
    "\n",
    "# Filter data for the first 30ns\n",
    "mask_original = df['original_time'] <= time_limit\n",
    "mask_resampled = df_resampled['time'] <= time_limit\n",
    "\n",
    "# Create comparison plot\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Top plot: Original signal\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(df.loc[mask_original, 'original_time'] * 1e9, \n",
    "        df.loc[mask_original, 'original_data'], \n",
    "        'b-', linewidth=1, alpha=0.8, label='Original Signal')\n",
    "plt.xlabel('Time (ns)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Original Signal - First 30ns')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Bottom plot: Signal comparison\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(df.loc[mask_original, 'original_time'] * 1e9, \n",
    "        df.loc[mask_original, 'original_data'], \n",
    "        'b-', linewidth=1, alpha=0.6, label=f'Original Signal')\n",
    "plt.plot(df_resampled.loc[mask_resampled, 'time'] * 1e9, \n",
    "        df_resampled.loc[mask_resampled, 'data'], \n",
    "        'r-', linewidth=1, alpha=0.8, label=f'Resampled Signal ({config.signal.fs_num} GHz)')\n",
    "plt.xlabel('Time (ns)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title(f'Signal Comparison - First 30ns (Original vs {config.signal.fs_num} GHz Resampled)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(f\"\\n=== First 30ns Signal Statistics ===\")\n",
    "print(f\"Original signal points: {mask_original.sum()}\")\n",
    "print(f\"Resampled signal points: {mask_resampled.sum()}\")\n",
    "print(f\"Original sampling rate: {1/df['original_time'].diff().mean()/1e9:.2f} GHz (estimated)\")\n",
    "print(f\"Resampled rate: {target_frequency_hz/1e9:.2f} GHz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d2de0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the target resampling frequency and interval\n",
    "target_frequency_hz = config.signal.up_fs\n",
    "resampling_interval_s = 1 / target_frequency_hz\n",
    "\n",
    "# Read the CSV file, skipping the original header row to replace it later\n",
    "df = pd.read_csv(config.csv.pn_t0, header=0)\n",
    "\n",
    "# Rename columns for clarity based on the original header structure\n",
    "# Assuming the first column is time-like and second is data-like\n",
    "df.columns = ['original_time', 'original_data']\n",
    "\n",
    "# Convert columns to numeric, coercing errors if any\n",
    "df['original_time'] = pd.to_numeric(df['original_time'], errors='coerce')\n",
    "df['original_data'] = pd.to_numeric(df['original_data'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values that might have resulted from coercion\n",
    "df.dropna(subset=['original_time', 'original_data'], inplace=True)\n",
    "\n",
    "# Prepare for resampling\n",
    "# The new time axis will start from 0 (because df['original_time'] now starts from 0)\n",
    "# and go up to the maximum duration of the adjusted time\n",
    "start_resample_time = 0\n",
    "end_resample_time = df['original_time'].max()\n",
    "\n",
    "new_time_axis = np.arange(start_resample_time, end_resample_time, resampling_interval_s)\n",
    "\n",
    "# Perform linear interpolation\n",
    "# np.interp needs the original x-values (df['time']) to be sorted\n",
    "# Assert that the data is already sorted by time\n",
    "assert df['original_time'].is_monotonic_increasing, \"Data must be sorted by time\"\n",
    "resampled_data_values = np.interp(new_time_axis, df['original_time'], df['original_data'])\n",
    "\n",
    "# Create a new DataFrame for the resampled data\n",
    "df_upsampled = pd.DataFrame({'time': new_time_axis, 'data': resampled_data_values})\n",
    "\n",
    "# Display the resampled data information without saving to file\n",
    "print(f\"Data resampled to {config.signal.up_fs_num} GHz.\")\n",
    "print(df_upsampled.head())\n",
    "\n",
    "# Display comparison of original and resampled signals for the first 30ns\n",
    "time_limit = 1e-8  # 10 ns\n",
    "\n",
    "# Filter data for the first 30ns\n",
    "mask_original = df['original_time'] <= time_limit\n",
    "mask_upsampled = df_upsampled['time'] <= time_limit\n",
    "\n",
    "\n",
    "\n",
    "# temp！！！\n",
    "\n",
    "# df_upsampled = df_upsampled[df_upsampled['time'] >= 0]\n",
    "# Extract data points with time >= 1.0781e-09\n",
    "df_upsampled = df_upsampled[df_upsampled['time'] >= 1.0881e-09]\n",
    "df_upsampled['time'] = df_upsampled['time'] - 1.0781e-09\n",
    "df_upsampled.reset_index(drop=True, inplace=True)\n",
    "\n",
    "plt.plot(df_upsampled['time'][:2000], df_upsampled['data'][:2000])\n",
    "plt.show()\n",
    "\n",
    "df_upsampled.describe()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create comparison plot\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Top plot: Original signal\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(df.loc[mask_original, 'original_time'] * 1e9, \n",
    "        df.loc[mask_original, 'original_data'], \n",
    "        'b-', linewidth=1, alpha=0.8, label='Original Signal')\n",
    "plt.xlabel('Time (ns)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Original Signal - First 30ns')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Bottom plot: Signal comparison\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(df.loc[mask_original, 'original_time'] * 1e9, \n",
    "        df.loc[mask_original, 'original_data'], \n",
    "        'b-', linewidth=1, alpha=0.6, label=f'Original Signal')\n",
    "plt.plot(df_upsampled.loc[mask_upsampled, 'time'] * 1e9, \n",
    "        df_upsampled.loc[mask_upsampled, 'data'], \n",
    "        'r-', linewidth=1, alpha=0.8, label=f'Resampled Signal ({config.signal.up_fs_num} GHz)')\n",
    "plt.xlabel('Time (ns)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title(f'Signal Comparison - First 30ns (Original vs {config.signal.up_fs_num} GHz Resampled)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(f\"\\n=== First 30ns Signal Statistics ===\")\n",
    "print(f\"Original signal points: {mask_original.sum()}\")\n",
    "print(f\"Resampled signal points: {mask_upsampled.sum()}\")\n",
    "print(f\"Original sampling rate: {1/df['original_time'].diff().mean()/1e9:.2f} GHz (estimated)\")\n",
    "print(f\"Resampled rate: {target_frequency_hz/1e9:.2f} GHz\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06794ce",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe27f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the square of the data to find power\n",
    "df_resampled['Data_Squared'] = df_resampled['data'] ** 2\n",
    "\n",
    "# Define the period for folding (assuming 500 Mbps symbol rate -> 2 ns period)\n",
    "period = 2e-9  # 2 ns\n",
    "\n",
    "# Use the squared data for folding analysis\n",
    "data_to_fold = df_resampled['Data_Squared'].dropna()\n",
    "time_to_fold = df_resampled.loc[data_to_fold.index, 'time']\n",
    "\n",
    "# Calculate the time modulo the period\n",
    "folded_time = time_to_fold % period\n",
    "\n",
    "# Determine the time resolution\n",
    "time_resolution = time_to_fold.diff().mean()\n",
    "if pd.isna(time_resolution):\n",
    "    time_resolution = (df_resampled['time'].iloc[1] - df_resampled['time'].iloc[0]) if len(df_resampled['time']) > 1 else 1e-12\n",
    "\n",
    "# Create bins for the 0-2ns range\n",
    "num_bins = max(1, int(period / time_resolution))\n",
    "bins = np.linspace(0, period, num_bins + 1)\n",
    "\n",
    "# Create a dataframe for folding\n",
    "fold_df = pd.DataFrame({'time': folded_time, 'data': data_to_fold})\n",
    "\n",
    "# Digitize the folded time to assign each time point to a bin\n",
    "fold_df['time_bin'] = pd.cut(fold_df['time'], bins=bins, labels=False, include_lowest=True)\n",
    "\n",
    "# Group by the bins and sum the data\n",
    "summed_data = fold_df.groupby('time_bin')['data'].sum()\n",
    "\n",
    "# Create the time axis for the summed data (using the middle of each bin)\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "# Reindex the summed data to match the bins, filling missing bins with 0\n",
    "summed_data = summed_data.reindex(range(len(bin_centers)), fill_value=0)\n",
    "\n",
    "# Normalize the summed data to be between 0 and 1\n",
    "summed_data = (summed_data - summed_data.min()) / (summed_data.max() - summed_data.min())\n",
    "\n",
    "# Plot the folded result\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(bin_centers, summed_data)\n",
    "plt.xlabel('Time (s) within 2ns period')\n",
    "plt.ylabel('Summed Data (Normalized)')\n",
    "plt.title('Data folded and summed over a 2ns period')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find the midpoint of the interval where data is above a threshold\n",
    "threshold = 0.2\n",
    "\n",
    "# Find where data is greater than threshold\n",
    "above_threshold = summed_data > threshold\n",
    "\n",
    "if above_threshold.any():\n",
    "    # Get the time values for these points\n",
    "    time_above_threshold = bin_centers[above_threshold]\n",
    "\n",
    "    # Calculate the circular mean of the time points to handle wrap-around\n",
    "    # Convert time to angles (radians)\n",
    "    angles = (time_above_threshold / period) * 2 * np.pi\n",
    "\n",
    "    # Compute the mean of the sines and cosines of the angles\n",
    "    mean_sin = np.mean(np.sin(angles))\n",
    "    mean_cos = np.mean(np.cos(angles))\n",
    "\n",
    "    # Calculate the mean angle from the mean sine and cosine\n",
    "    mean_angle = np.arctan2(mean_sin, mean_cos)\n",
    "\n",
    "    # Convert the mean angle back to time\n",
    "    midpoint_time = (mean_angle / (2 * np.pi)) * period\n",
    "\n",
    "    # Adjust the midpoint to be in the [0, period] range\n",
    "    if midpoint_time < 0:\n",
    "        midpoint_time += period\n",
    "\n",
    "    print(f\"The midpoint of the interval where data is > {threshold} is: {midpoint_time:.4e} s\")\n",
    "    print(f\"Need to delay the signal by {period - midpoint_time:.4e} s to align with the end of period.\")\n",
    "    print(f\"This corresponds to {int((period - midpoint_time) * config.signal.fs)} samples at {config.signal.fs_num} GHz sampling rate.\")\n",
    "\n",
    "    # Plot the result with midpoint visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(bin_centers, summed_data, label='Summed Data')\n",
    "    plt.axhline(y=threshold, color='r', linestyle='--', label=f'Threshold ({threshold})')\n",
    "    plt.axvline(x=midpoint_time, color='g', linestyle='-', label=f'Midpoint ({midpoint_time:.2e} s)')\n",
    "    plt.xlabel('Time (s) within 2ns period')\n",
    "    plt.ylabel('Summed Data (Normalized)')\n",
    "    plt.title('Data folded and summed over a 2ns period with Midpoint')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(f\"No data points found above the threshold of {threshold}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dff7e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first (midpoint_time + 1e-9) seconds of data\n",
    "# Calculate the time threshold\n",
    "time_threshold = midpoint_time + 1e-9  # Add 1 ns to midpoint_time\n",
    "\n",
    "# Find the index where time first exceeds the threshold\n",
    "indices_to_remove = df_resampled['time'] < time_threshold\n",
    "num_points_to_remove = indices_to_remove.sum()\n",
    "\n",
    "print(f\"Time threshold: {time_threshold:.4e} s\")\n",
    "print(f\"Number of data points to remove: {num_points_to_remove}\")\n",
    "\n",
    "# Create the trimmed dataframe\n",
    "df_trimmed = df_resampled[~indices_to_remove].copy()\n",
    "\n",
    "# Reset the time axis to start from 0 again\n",
    "df_trimmed['time'] = df_trimmed['time'] - df_trimmed['time'].min()\n",
    "\n",
    "# Reset the index\n",
    "df_trimmed.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Original data shape: {df_resampled.shape}\")\n",
    "print(f\"Trimmed data shape: {df_trimmed.shape}\")\n",
    "print(f\"Data points removed: {df_resampled.shape[0] - df_trimmed.shape[0]}\")\n",
    "print(f\"First few rows of trimmed data:\")\n",
    "print(df_trimmed.head())\n",
    "\n",
    "# Plot the trimmed data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_trimmed['time'], df_trimmed['data'])\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Data')\n",
    "plt.title('Trimmed Data vs Time')\n",
    "plt.grid(True)\n",
    "plt.xlim(0, 1e-8)  # Show 0-10 ns\n",
    "# Set x-axis ticks every 2 ns\n",
    "plt.xticks(np.arange(0, 1e-8 + 2e-9, 2e-9))\n",
    "plt.show()\n",
    "\n",
    "# Save the trimmed data to CSV file\n",
    "# Only save time and data columns (exclude Data_Squared)\n",
    "df_to_save = pd.DataFrame(df_trimmed[['time', 'data']])\n",
    "df_to_save.to_csv(config.csv.fs, index=False)\n",
    "print(f\"Trimmed data saved to {config.csv.fs}\")\n",
    "print(f\"Saved data shape: {df_to_save.shape}\")\n",
    "print(f\"Columns saved: {list(df_to_save.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92c3331",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_awgn = pd.DataFrame(df_trimmed[['time', 'data']])\n",
    "\n",
    "# Generate AWGN noise\n",
    "noise = np.random.normal(loc=0, scale=np.sqrt(config.awgn.noise_power), size=len(df_awgn))\n",
    "\n",
    "# Add noise to the signal\n",
    "df_awgn['data'] = df_awgn['data'] + noise\n",
    "\n",
    "print(f\"Added AWGN noise: SNR={config.awgn.snr_db} dB (linear={config.awgn.snr_linear:.2f}), signal power={config.awgn.signal_power}, noise power={config.awgn.noise_power:.2e}\")\n",
    "print(df_awgn.head())\n",
    "\n",
    "\n",
    "noise = np.random.normal(loc=0, scale=np.sqrt(config.awgn.noise_power), size=len(df_upsampled))\n",
    "df_upsampled['data'] = df_upsampled['data'] + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3122c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Method 1: FFT-based zero-padding upsampling from fs_num GHz to 2048 GHz\n",
    "# print(f\"=== FFT Zero-Padding Upsampling: {config.signal.fs_num} GHz → {config.signal.up_fs_num} GHz ===\")\n",
    "\n",
    "# # Use the loaded data from the CSV file\n",
    "# x_orig = df['data'].to_numpy(dtype=np.float64)\n",
    "# N = len(x_orig)\n",
    "# upsample_factor = int(config.signal.up_fs_num / config.signal.fs_num)\n",
    "# print(f\"Original data length: {N}\")\n",
    "# print(f\"Original sampling rate: {config.signal.fs_num} GHz\")\n",
    "# print(f\"Target sampling rate: {config.signal.up_fs_num} GHz\")\n",
    "# print(f\"Upsampling factor: {upsample_factor}×\")\n",
    "\n",
    "# # Step 1: FFT of original data\n",
    "# X = np.fft.fft(x_orig)\n",
    "\n",
    "# # Step 2: Create zero-padded frequency domain signal\n",
    "# N_new = N * upsample_factor\n",
    "# X_padded = np.zeros(N_new, dtype=complex)\n",
    "\n",
    "# # For even N: split the Nyquist frequency component\n",
    "# if N % 2 == 0:\n",
    "#     # Copy positive frequencies [0, N/2]\n",
    "#     X_padded[:N//2] = X[:N//2]\n",
    "#     # Copy negative frequencies [N/2+1, N-1] to the end\n",
    "#     X_padded[N_new-N//2+1:] = X[N//2+1:]\n",
    "#     # Split Nyquist frequency (if it exists)\n",
    "#     X_padded[N//2] = X[N//2] / 2\n",
    "#     X_padded[N_new-N//2] = X[N//2] / 2\n",
    "# else:\n",
    "#     # For odd N: simpler case\n",
    "#     X_padded[:(N+1)//2] = X[:(N+1)//2]\n",
    "#     X_padded[N_new-(N-1)//2:] = X[(N+1)//2:]\n",
    "\n",
    "# # Step 3: IFFT to get upsampled signal\n",
    "# x_upsampled = np.fft.ifft(X_padded).real * upsample_factor  # Scale by upsampling factor\n",
    "\n",
    "# # Step 4: Create precise time axis\n",
    "# Ts_orig = 1 / config.signal.fs  # Original sampling period\n",
    "# Ts_upsampled = Ts_orig / upsample_factor  # New sampling period\n",
    "# t_upsampled = np.arange(len(x_upsampled), dtype=np.float64) * Ts_upsampled\n",
    "\n",
    "# print(f\"Upsampled data length: {len(x_upsampled)}\")\n",
    "# print(f\"New sampling period: {Ts_upsampled:.2e} s\")\n",
    "# print(f\"New sampling rate: {1/Ts_upsampled/1e9:.1f} GHz\")\n",
    "\n",
    "# # Create DataFrame for the upsampled data\n",
    "# df_upsampled = pd.DataFrame({\n",
    "#     'time': t_upsampled,\n",
    "#     'data': x_upsampled\n",
    "# })\n",
    "\n",
    "# print(f\"Upsampled data shape: {df_upsampled.shape}\")\n",
    "# print(f\"First few rows of {config.signal.up_fs_num} GHz data:\")\n",
    "# print(df_upsampled.head())\n",
    "\n",
    "# # Verify the upsampling quality by checking the time axis\n",
    "# print(f\"\\nTime axis verification:\")\n",
    "# print(f\"Original max time: {df['time'].max():.2e} s\")\n",
    "# print(f\"Upsampled max time: {t_upsampled.max():.2e} s\")\n",
    "# print(f\"Time axis ratio: {t_upsampled.max() / df['time'].max():.6f} (should be close to 1.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f0ab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsample_factor = int(config.signal.up_fs_num / config.signal.fs_num)\n",
    "\n",
    "# x_orig = df['data'].to_numpy(dtype=np.float64)\n",
    "# x_upsampled = np.zeros(x_orig.shape[0] * upsample_factor)\n",
    "# x_upsampled[::upsample_factor] = x_orig * upsample_factor\n",
    "\n",
    "# Ts_orig = 1 / config.signal.fs  # Original sampling period\n",
    "# Ts_upsampled = Ts_orig / upsample_factor  # New sampling period\n",
    "# t_upsampled = np.arange(len(x_upsampled), dtype=np.float64) * Ts_upsampled\n",
    "\n",
    "# df_upsampled = pd.DataFrame({\n",
    "#     'time': t_upsampled,\n",
    "#     'data': x_upsampled\n",
    "# })\n",
    "\n",
    "# plt.plot(df_upsampled['time'][:2000], df_upsampled['data'][:2000])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d920fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsample_factor = int(config.signal.up_fs_num / config.signal.fs_num)\n",
    "\n",
    "# Plot comparison between original and upsampled data\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot 1: First 1e-8 seconds comparison\n",
    "plt.subplot(2, 1, 1)\n",
    "time_limit = 1e-8\n",
    "\n",
    "# fs_num GHz data\n",
    "mask_orig = df_awgn['time'] <= time_limit\n",
    "plt.plot(df_awgn.loc[mask_orig, 'time'], df_awgn.loc[mask_orig, 'data'], \n",
    "         'o-', markersize=3, linewidth=1, label=f'{config.signal.fs_num} GHz (original)', alpha=0.7)\n",
    "\n",
    "# up_fs_num GHz data\n",
    "mask_upsampled = df_upsampled['time'] <= time_limit\n",
    "plt.plot(df_upsampled.loc[mask_upsampled, 'time'], df_upsampled.loc[mask_upsampled, 'data'], \n",
    "         '-', linewidth=0.8, label=f'{config.signal.up_fs_num} GHz (upsampled)', alpha=0.9)\n",
    "\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Data')\n",
    "plt.title(f'Comparison: {config.signal.fs_num} GHz vs {config.signal.up_fs_num} GHz Data (First 10 ns)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.xlim(0, time_limit)\n",
    "plt.xticks(np.arange(0, time_limit + 2e-9, 2e-9))\n",
    "\n",
    "# Plot 2: Zoomed view of one pulse\n",
    "plt.subplot(2, 1, 2)\n",
    "time_start = 0\n",
    "time_end = 2e-9  # First 2 ns only\n",
    "\n",
    "mask_orig_zoom = (df_awgn['time'] >= time_start) & (df_awgn['time'] <= time_end)\n",
    "mask_upsampled_zoom = (df_upsampled['time'] >= time_start) & (df_upsampled['time'] <= time_end)\n",
    "\n",
    "plt.plot(df_awgn.loc[mask_orig_zoom, 'time'], df_awgn.loc[mask_orig_zoom, 'data'], \n",
    "         'o-', markersize=4, linewidth=1.5, label=f'{config.signal.fs_num} GHz (original)')\n",
    "\n",
    "plt.plot(df_upsampled.loc[mask_upsampled_zoom, 'time'], df_upsampled.loc[mask_upsampled_zoom, 'data'], \n",
    "         '-', linewidth=1, label=f'{config.signal.up_fs_num} GHz (upsampled)')\n",
    "\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Data')\n",
    "plt.title('Zoomed View: Single Pulse (First 2 ns)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.xlim(time_start, time_end)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the up_fs_num GHz data to CSV\n",
    "df_upsampled.to_csv(config.csv.up_fs, index=False)\n",
    "print(f\"\\n{config.signal.up_fs_num} GHz upsampled data saved to: {config.csv.up_fs}\")\n",
    "print(f\"File size: ~{len(df_upsampled) * 2 * 8 / 1024**2:.1f} MB\")\n",
    "\n",
    "# Calculate some quality metrics\n",
    "print(f\"\\nUpsampling quality check:\")\n",
    "print(f\"Original data points: {len(df_awgn)}\")\n",
    "print(f\"Upsampled data points: {len(df_upsampled)} (should be {len(df_awgn) * upsample_factor})\")\n",
    "print(f\"Ratio: {len(df_upsampled) / len(df_awgn):.1f} (should be {upsample_factor:.1f})\")\n",
    "\n",
    "# Check if the upsampled data preserves the original samples\n",
    "original_indices = np.arange(0, len(df_upsampled), upsample_factor)\n",
    "if len(original_indices) <= len(df_awgn):\n",
    "    max_diff = np.max(np.abs(df_upsampled.iloc[original_indices]['data'].values[:len(df_awgn)] - df_awgn['data'].values))\n",
    "    print(f\"Maximum difference at original sample points: {max_diff:.2e} (should be ~0)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4349ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(config.csv.up_fs)\n",
    "\n",
    "print(f\"Reading file: {config.csv.up_fs}\")  # Add this line to confirm which file is being read\n",
    "\n",
    "# Plot Data column vs Time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['time'][:10000], df['data'][:10000])\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Data')\n",
    "plt.title('Data vs Time')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Calculate the square of the data\n",
    "df['Data_Squared'] = df['data'] ** 2\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['time'][:10000], df['Data_Squared'][:10000])\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Data Squared')\n",
    "plt.title('Data Squared vs Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adbebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Butterworth low-pass filter parameters\n",
    "native_sampling_rate = config.signal.up_fs  # Use the upsampled frequency\n",
    "cutoff = 2e9  # 4 GHz cutoff frequency\n",
    "N = 4  # Filter order\n",
    "nyq = native_sampling_rate / 2\n",
    "cutoff_norm = cutoff / nyq\n",
    "\n",
    "# Design Butterworth filter\n",
    "b, a = sp.signal.butter(N, cutoff_norm, btype='low')\n",
    "\n",
    "# Apply zero-phase filtering\n",
    "df['Data_MA'] = sp.signal.filtfilt(b, a, df['Data_Squared'])\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.plot(df['time'][:10000], df['Data_Squared'][:10000], label='Data Squared', alpha=0.7)\n",
    "plt.plot(df['time'][:10000], df['Data_MA'][:10000], label=f'Butterworth LPF (N={N})', linewidth=2)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Butterworth Low-pass Filter Effect')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5fec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set threshold\n",
    "threshold = 0.1\n",
    "native_sampling_rate = config.signal.up_fs  # Use the upsampled frequency\n",
    "symbol_rate = config.signal.fsymbol  # 500 MHz\n",
    "\n",
    "# Get data\n",
    "filtered_data = df[\"Data_MA\"].values\n",
    "\n",
    "# Calculate both rising and falling edges\n",
    "above = filtered_data > threshold\n",
    "rising_edges = np.where(np.diff(above.astype(int)) == 1)[0] + 1   # Rising edges\n",
    "falling_edges = np.where(np.diff(above.astype(int)) == -1)[0] + 1  # Falling edges\n",
    "\n",
    "print(f\"Found {len(rising_edges)} rising edges\")\n",
    "print(f\"Found {len(falling_edges)} falling edges\")\n",
    "\n",
    "# Calculate midpoints between rising and falling edges\n",
    "mid_indices = []\n",
    "mid_times = []\n",
    "\n",
    "if rising_edges.size > 0 and falling_edges.size > 0:\n",
    "    # Find pairs of rising and falling edges to calculate midpoints\n",
    "    i_rising = 0\n",
    "    i_falling = 0\n",
    "    \n",
    "    while i_rising < len(rising_edges) and i_falling < len(falling_edges):\n",
    "        rise_idx = rising_edges[i_rising]\n",
    "        fall_idx = falling_edges[i_falling]\n",
    "        \n",
    "        # Find the next falling edge after the current rising edge\n",
    "        if fall_idx > rise_idx:\n",
    "            # Calculate midpoint between this rising edge and falling edge\n",
    "            midpoint_idx = int((rise_idx + fall_idx) / 2)\n",
    "            mid_indices.append(midpoint_idx)\n",
    "            mid_times.append(df.loc[midpoint_idx, 'time'])\n",
    "            i_rising += 1\n",
    "            i_falling += 1\n",
    "        else:\n",
    "            # This falling edge is before the current rising edge, skip it\n",
    "            i_falling += 1\n",
    "    \n",
    "    mid_indices = np.array(mid_indices)\n",
    "    mid_times = np.array(mid_times)\n",
    "    \n",
    "    print(f\"Found {len(mid_indices)} valid midpoints between rising and falling edges\")\n",
    "    if len(mid_times) > 0:\n",
    "        print(f\"First 20 midpoint times: {mid_times[:20]}\")\n",
    "else:\n",
    "    mid_indices = np.array([])\n",
    "    mid_times = np.array([])\n",
    "    print(\"No rising/falling edge pairs detected\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df['time'][:20480*2], df['Data_MA'][:20480*2], label='Filtered Data (Data_MA)')\n",
    "plt.axhline(y=threshold, color='gray', linestyle='--', alpha=0.5, label=f'Threshold ({threshold})')\n",
    "\n",
    "if rising_edges.size > 0:\n",
    "    plt.plot(df.loc[rising_edges, 'time'][:20], filtered_data[rising_edges][:20], '^g', markersize=8, label='Rising Edge')\n",
    "if falling_edges.size > 0:\n",
    "    plt.plot(df.loc[falling_edges, 'time'][:20], filtered_data[falling_edges][:20], 'vr', markersize=8, label='Falling Edge')\n",
    "if mid_indices.size > 0:\n",
    "    plt.plot(df.loc[mid_indices, 'time'][:20], filtered_data[mid_indices][:20], 'ko', markersize=6, label='Midpoints')\n",
    "\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Filtered Data (Moving Average)')\n",
    "plt.title('Rising/Falling Edges and Their Midpoints')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d92c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fold the lowpass filtered data by symbol duration\n",
    "# # and check rising/falling edges for each symbol\n",
    "\n",
    "# print(\"=== Folding Low-pass Filtered Data by Symbol Duration ===\")\n",
    "\n",
    "# # Get the symbol duration in samples\n",
    "# symbol_duration_samples = int(config.signal.up_fs * config.signal.symbol_duration)\n",
    "# print(f\"Symbol duration: {config.signal.symbol_duration:.2e} s\")\n",
    "# print(f\"Symbol duration in samples: {symbol_duration_samples}\")\n",
    "\n",
    "# # Get the filtered data\n",
    "# filtered_data = df['Data_MA'].values\n",
    "# data_length = len(filtered_data)\n",
    "\n",
    "# print(f\"Total data length: {data_length} samples\")\n",
    "# print(f\"Expected symbols: {data_length / symbol_duration_samples:.2f}\")\n",
    "\n",
    "# # Calculate number of complete symbols and trim data if necessary\n",
    "# num_symbols = data_length // symbol_duration_samples\n",
    "# trimmed_length = num_symbols * symbol_duration_samples\n",
    "\n",
    "# if data_length % symbol_duration_samples != 0:\n",
    "#     print(f\"⚠ Data length ({data_length}) is not divisible by symbol duration ({symbol_duration_samples})\")\n",
    "#     print(f\"Trimming data to {trimmed_length} samples ({num_symbols} complete symbols)\")\n",
    "#     filtered_data = filtered_data[:trimmed_length]\n",
    "#     data_length = trimmed_length\n",
    "\n",
    "# print(f\"Using data length: {data_length} samples\")\n",
    "# print(f\"Number of complete symbols: {num_symbols}\")\n",
    "\n",
    "# # Now assert that the trimmed data length is exactly divisible by symbol duration\n",
    "# assert data_length % symbol_duration_samples == 0, \\\n",
    "#     f\"Trimmed data length ({data_length}) is not divisible by symbol duration ({symbol_duration_samples})\"\n",
    "\n",
    "# # Fold the data - reshape to (num_symbols, symbol_duration_samples)\n",
    "# folded_data = filtered_data.reshape(num_symbols, symbol_duration_samples)\n",
    "# print(f\"Folded data shape: {folded_data.shape}\")\n",
    "\n",
    "# # Check rising and falling edges for each symbol\n",
    "# threshold = 0.1  # Same threshold as before\n",
    "# rising_edges_per_symbol = []\n",
    "# falling_edges_per_symbol = []\n",
    "# midpoints_per_symbol = []\n",
    "\n",
    "# for symbol_idx in range(num_symbols):\n",
    "#     symbol_data = folded_data[symbol_idx]\n",
    "    \n",
    "#     # Find rising and falling edges within this symbol\n",
    "#     above = symbol_data > threshold\n",
    "#     rising_edges = np.where(np.diff(above.astype(int)) == 1)[0] + 1\n",
    "#     falling_edges = np.where(np.diff(above.astype(int)) == -1)[0] + 1\n",
    "    \n",
    "#     rising_edges_per_symbol.append(rising_edges)\n",
    "#     falling_edges_per_symbol.append(falling_edges)\n",
    "    \n",
    "#     # Calculate midpoint for this symbol (400 samples left of falling edge)\n",
    "#     if len(falling_edges) > 0:\n",
    "#         midpoint = falling_edges[0] - 400\n",
    "#         if midpoint >= 0:\n",
    "#             midpoints_per_symbol.append(midpoint)\n",
    "#         else:\n",
    "#             midpoints_per_symbol.append(0)  # If midpoint would be negative, use 0\n",
    "#     else:\n",
    "#         midpoints_per_symbol.append(None)  # No falling edge found\n",
    "\n",
    "# # Print statistics\n",
    "# print(f\"\\n=== Edge Detection Statistics ===\")\n",
    "# rising_counts = [len(edges) for edges in rising_edges_per_symbol]\n",
    "# falling_counts = [len(edges) for edges in falling_edges_per_symbol]\n",
    "\n",
    "# print(f\"Rising edges per symbol - Min: {min(rising_counts)}, Max: {max(rising_counts)}, Mean: {np.mean(rising_counts):.2f}\")\n",
    "# print(f\"Falling edges per symbol - Min: {min(falling_counts)}, Max: {max(falling_counts)}, Mean: {np.mean(falling_counts):.2f}\")\n",
    "\n",
    "# # Count symbols with exactly 1 rising and 1 falling edge\n",
    "# perfect_symbols = sum(1 for i in range(num_symbols) if rising_counts[i] == 1 and falling_counts[i] == 1)\n",
    "# print(f\"Symbols with exactly 1 rising and 1 falling edge: {perfect_symbols}/{num_symbols} ({perfect_symbols/num_symbols*100:.1f}%)\")\n",
    "\n",
    "# # Assert that each symbol has exactly one rising and one falling edge\n",
    "# try:\n",
    "#     for i in range(num_symbols):\n",
    "#         assert rising_counts[i] == 1, f\"Symbol {i} has {rising_counts[i]} rising edges (expected 1)\"\n",
    "#         assert falling_counts[i] == 1, f\"Symbol {i} has {falling_counts[i]} falling edges (expected 1)\"\n",
    "#     print(\"✓ All symbols have exactly 1 rising and 1 falling edge\")\n",
    "# except AssertionError as e:\n",
    "#     print(f\"⚠ Warning: {e}\")\n",
    "#     print(\"Not all symbols have exactly 1 rising and 1 falling edge\")\n",
    "\n",
    "# # Visualize the first few symbols\n",
    "# plt.figure(figsize=(15, 10))\n",
    "\n",
    "# # Plot first 8 symbols\n",
    "# for i in range(min(8, num_symbols)):\n",
    "#     plt.subplot(2, 4, i+1)\n",
    "#     symbol_data = folded_data[i]\n",
    "#     x_axis = np.arange(len(symbol_data))\n",
    "    \n",
    "#     plt.plot(x_axis, symbol_data, 'b-', alpha=0.7, label='Data')\n",
    "#     plt.axhline(y=threshold, color='r', linestyle='--', alpha=0.5, label='Threshold')\n",
    "    \n",
    "#     # Plot rising edges\n",
    "#     if len(rising_edges_per_symbol[i]) > 0:\n",
    "#         for edge in rising_edges_per_symbol[i]:\n",
    "#             plt.axvline(x=edge, color='g', linestyle='-', alpha=0.7, label='Rising' if edge == rising_edges_per_symbol[i][0] else '')\n",
    "    \n",
    "#     # Plot falling edges\n",
    "#     if len(falling_edges_per_symbol[i]) > 0:\n",
    "#         for edge in falling_edges_per_symbol[i]:\n",
    "#             plt.axvline(x=edge, color='r', linestyle='-', alpha=0.7, label='Falling' if edge == falling_edges_per_symbol[i][0] else '')\n",
    "    \n",
    "#     # Plot midpoint\n",
    "#     if midpoints_per_symbol[i] is not None:\n",
    "#         plt.axvline(x=midpoints_per_symbol[i], color='k', linestyle=':', alpha=0.8, label='Midpoint')\n",
    "    \n",
    "#     plt.title(f'Symbol {i}\\nR:{len(rising_edges_per_symbol[i])}, F:{len(falling_edges_per_symbol[i])}')\n",
    "#     plt.grid(True, alpha=0.3)\n",
    "#     if i == 0:\n",
    "#         plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Store results for later use\n",
    "# print(f\"\\n=== Results Summary ===\")\n",
    "# print(f\"Folded data shape: {folded_data.shape}\")\n",
    "# print(f\"Midpoints per symbol: {len([mp for mp in midpoints_per_symbol if mp is not None])}/{num_symbols}\")\n",
    "# print(f\"First 10 midpoints: {midpoints_per_symbol[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f68f92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all mid positions and use K-means clustering\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Configuration parameters\n",
    "N_CLUSTERS = 2  # Number of clusters for K-means analysis\n",
    "\n",
    "# Extract mid positions from the previous analysis\n",
    "if 'mid_indices' in locals() and 'mid_times' in locals():\n",
    "    # List of mid indices in the dataframe\n",
    "    mid_indices_list = mid_indices.tolist()\n",
    "    \n",
    "    # List of mid time positions\n",
    "    mid_positions = mid_times.tolist()\n",
    "    \n",
    "    print(\"Mid Point Analysis Results:\")\n",
    "    print(f\"Number of mid points found: {len(mid_positions)}\")\n",
    "    print(f\"Mid indices: {mid_indices_list}\")\n",
    "    print(f\"Mid time positions (seconds): {mid_positions}\")\n",
    "    \n",
    "    # Convert to more readable format (nanoseconds)\n",
    "    mid_positions_ns = [pos * 1e9 for pos in mid_positions]\n",
    "    print(f\"Mid time positions (nanoseconds): {mid_positions_ns}\")\n",
    "    \n",
    "    # Calculate relative positions from the first mid point\n",
    "    if len(mid_positions) > 1:\n",
    "        relative_positions = [(pos - mid_positions[0]) * 1e9 for pos in mid_positions]\n",
    "        print(f\"Relative positions from first mid point (ns): {relative_positions}\")\n",
    "        \n",
    "        # Calculate intervals between consecutive mid points\n",
    "        intervals_ns = [pos * 1e9 for pos in np.diff(mid_positions)]\n",
    "        print(f\"Intervals between consecutive mid points (ns): {intervals_ns}\")\n",
    "        \n",
    "    # Use K-means clustering on mid positions (modulo 2ns for periodic analysis)\n",
    "    if len(mid_positions_ns) > N_CLUSTERS:  # Need at least N_CLUSTERS points for clustering\n",
    "        mid_positions_mod = np.array([pos % 2 for pos in mid_positions_ns]).reshape(-1, 1)\n",
    "        \n",
    "        # Apply K-means with N_CLUSTERS clusters\n",
    "        kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=42)\n",
    "        cluster_labels_original = kmeans.fit_predict(mid_positions_mod)\n",
    "        cluster_centers = kmeans.cluster_centers_.flatten()\n",
    "        \n",
    "        # Create mapping to sort clusters by time position\n",
    "        center_time_pairs = [(i, center) for i, center in enumerate(cluster_centers)]\n",
    "        center_time_pairs.sort(key=lambda x: x[1])  # Sort by time position\n",
    "        \n",
    "        # Create mapping from original cluster ID to time-sorted cluster ID\n",
    "        old_to_new_mapping = {}\n",
    "        for new_id, (old_id, _) in enumerate(center_time_pairs):\n",
    "            old_to_new_mapping[old_id] = new_id\n",
    "        \n",
    "        # Apply the mapping to cluster labels\n",
    "        cluster_labels = np.array([old_to_new_mapping[label] for label in cluster_labels_original])\n",
    "        \n",
    "        # Sort cluster centers by time position\n",
    "        sorted_centers = [pair[1] for pair in center_time_pairs]\n",
    "        \n",
    "        print(f\"\\n=== K-means Clustering Results ({N_CLUSTERS} clusters) - Time Sorted ===\")\n",
    "        for i, center in enumerate(sorted_centers):\n",
    "            cluster_size = np.sum(cluster_labels == i)\n",
    "            print(f\"Cluster {i}: Time = {center:.4f} ns (modulo 2), Points = {cluster_size}\")\n",
    "        \n",
    "        # Convert cluster centers to picoseconds and display as list\n",
    "        print(f\"\\nCluster centers in picoseconds (time sorted): [\", end=\"\")\n",
    "        for i in range(len(sorted_centers)):\n",
    "            print(f\"{sorted_centers[i] * 1000:.2f}\", end=\" \" if i < len(sorted_centers) - 1 else \"\")\n",
    "        print(\"]\")\n",
    "\n",
    "        # Print a nanosecond list\n",
    "        print(f\"\\nCluster centers in nanoseconds (time sorted): [\", end=\"\")\n",
    "        for i in range(len(sorted_centers)):\n",
    "            print(f\"{sorted_centers[i]:.8f}\", end=\" \" if i < len(sorted_centers) - 1 else \"\")\n",
    "        print(\"]\")\n",
    "\n",
    "        # Calculate cluster positions (for 2 clusters)\n",
    "        first_cluster_center = sorted_centers[0]\n",
    "        second_cluster_center = sorted_centers[1]\n",
    "        \n",
    "        print(f\"\\n=== Cluster Position Analysis ===\")\n",
    "        print(f\"Position of cluster 0: {first_cluster_center:.6f} ns\")\n",
    "        print(f\"Position of cluster 1: {second_cluster_center:.6f} ns\")\n",
    "        print(f\"Difference between the two clusters: {second_cluster_center - first_cluster_center:.6f} ns\")\n",
    "        \n",
    "        # Also show in picoseconds\n",
    "        print(f\"\\nPosition of cluster 0: {first_cluster_center * 1000:.2f} ps\")\n",
    "        print(f\"Position of cluster 1: {second_cluster_center * 1000:.2f} ps\")\n",
    "        print(f\"Difference between the two clusters: {(second_cluster_center - first_cluster_center) * 1000:.2f} ps\")\n",
    "        \n",
    "        # For compatibility with downstream code, assign the cluster centers to the old variable names\n",
    "        first_8_average = first_cluster_center\n",
    "        last_8_average = second_cluster_center\n",
    "\n",
    "        # Visualize clustering results\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        # Generate colors for clusters\n",
    "        colors = ['#1f77b4', '#ff7f0e']\n",
    "\n",
    "        # Plot cluster centers (now time-sorted)\n",
    "        plt.scatter(sorted_centers, range(N_CLUSTERS), color='red', marker='x', s=150, \n",
    "                   linewidths=3, label='Cluster Centers')\n",
    "        \n",
    "        # Plot cluster points\n",
    "        for i in range(N_CLUSTERS):\n",
    "            cluster_points = mid_positions_mod[cluster_labels == i]\n",
    "            plt.scatter(cluster_points, np.ones(len(cluster_points)) * i, \n",
    "                       color=colors[i % len(colors)], alpha=0.7, s=30, label=f'Cluster {i}')\n",
    "        \n",
    "        # Add vertical lines for cluster centers\n",
    "        plt.axvline(x=float(first_cluster_center), color='blue', linestyle='--', linewidth=3, alpha=0.8, \n",
    "                   label=f'Cluster 0 center: {first_cluster_center:.4f} ns')\n",
    "        plt.axvline(x=float(second_cluster_center), color='orange', linestyle='--', linewidth=3, alpha=0.8, \n",
    "                   label=f'Cluster 1 center: {second_cluster_center:.4f} ns')\n",
    "        \n",
    "        # Add text annotations for the cluster centers\n",
    "        plt.text(float(first_cluster_center) + 0.05, N_CLUSTERS/2 - 0.3, f'Cluster 0\\nCenter: {first_cluster_center:.4f} ns', \n",
    "                rotation=90, verticalalignment='center', fontsize=10, \n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='blue', alpha=0.1))\n",
    "        plt.text(float(second_cluster_center) + 0.05, N_CLUSTERS/2 + 0.3, f'Cluster 1\\nCenter: {second_cluster_center:.4f} ns', \n",
    "                rotation=90, verticalalignment='center', fontsize=10,\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='orange', alpha=0.1))\n",
    "        \n",
    "        plt.xlabel('Mid Position (ns, modulo 2)')\n",
    "        plt.ylabel('Cluster ID (Time Sorted)')\n",
    "        plt.title(f'K-means Clustering of Mid Positions ({N_CLUSTERS} clusters, Time Sorted)')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        print(f\"Not enough mid points for K-means clustering (found {len(mid_positions_ns)}, need > {N_CLUSTERS})\")\n",
    "        \n",
    "else:\n",
    "    print(\"Please run the mid point detection cell first to generate mid point data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2365379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified classification logic: directly use 1ns as boundary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration parameters\n",
    "THRESHOLD_NS = 1.0  # 1ns threshold for classification\n",
    "\n",
    "# Extract mid positions from the previous analysis\n",
    "if 'mid_indices' in locals() and 'mid_times' in locals():\n",
    "    # List of mid indices in the dataframe\n",
    "    mid_indices_list = mid_indices.tolist()\n",
    "    \n",
    "    # List of mid time positions\n",
    "    mid_positions = mid_times.tolist()\n",
    "    \n",
    "    print(\"Mid Point Analysis Results:\")\n",
    "    print(f\"Number of mid points found: {len(mid_positions)}\")\n",
    "    print(f\"Mid indices: {mid_indices_list}\")\n",
    "    print(f\"Mid time positions (seconds): {mid_positions}\")\n",
    "    \n",
    "    # Convert to more readable format (nanoseconds)\n",
    "    mid_positions_ns = [pos * 1e9 for pos in mid_positions]\n",
    "    print(f\"Mid time positions (nanoseconds): {mid_positions_ns}\")\n",
    "    \n",
    "    # Calculate relative positions from the first mid point\n",
    "    if len(mid_positions) > 1:\n",
    "        relative_positions = [(pos - mid_positions[0]) * 1e9 for pos in mid_positions]\n",
    "        print(f\"Relative positions from first mid point (ns): {relative_positions}\")\n",
    "        \n",
    "        # Calculate intervals between consecutive mid points\n",
    "        intervals_ns = [pos * 1e9 for pos in np.diff(mid_positions)]\n",
    "        print(f\"Intervals between consecutive mid points (ns): {intervals_ns}\")\n",
    "        \n",
    "    # Simplified classification: using fixed threshold 1ns\n",
    "    if len(mid_positions_ns) > 0:\n",
    "        # Calculate positions after modulo 2ns\n",
    "        mid_positions_mod = np.array([pos % 2 for pos in mid_positions_ns])\n",
    "        \n",
    "        # Directly use 1ns as threshold for classification\n",
    "        cluster_labels = np.where(mid_positions_mod < THRESHOLD_NS, 0, 1)\n",
    "        \n",
    "        # Calculate center position of each cluster\n",
    "        cluster_0_positions = mid_positions_mod[cluster_labels == 0]\n",
    "        cluster_1_positions = mid_positions_mod[cluster_labels == 1]\n",
    "        \n",
    "        if len(cluster_0_positions) > 0:\n",
    "            # Calculate the average of 5th and 95th percentiles as cluster center\n",
    "            p5 = np.percentile(cluster_0_positions, 5)\n",
    "            p95 = np.percentile(cluster_0_positions, 95)\n",
    "            first_cluster_center = (p5 + p95) / 2\n",
    "        else:\n",
    "            first_cluster_center = 0.5  # Default value\n",
    "            \n",
    "        if len(cluster_1_positions) > 0:\n",
    "            # Calculate the average of 5th and 95th percentiles as cluster center\n",
    "            p5 = np.percentile(cluster_1_positions, 5)\n",
    "            p95 = np.percentile(cluster_1_positions, 95)\n",
    "            second_cluster_center = (p5 + p95) / 2\n",
    "        else:\n",
    "            second_cluster_center = 1.5  # Default value\n",
    "            \n",
    "        sorted_centers = [first_cluster_center, second_cluster_center]\n",
    "        \n",
    "        print(f\"\\n=== Simplified Classification Results (Threshold: {THRESHOLD_NS} ns) ===\")\n",
    "        cluster_0_count = np.sum(cluster_labels == 0)\n",
    "        cluster_1_count = np.sum(cluster_labels == 1)\n",
    "        print(f\"Cluster 0: Time = {first_cluster_center:.4f} ns (modulo 2), Points = {cluster_0_count}\")\n",
    "        print(f\"Cluster 1: Time = {second_cluster_center:.4f} ns (modulo 2), Points = {cluster_1_count}\")\n",
    "        \n",
    "        # Maintain output format compatible with original code\n",
    "        print(f\"\\nCluster centers in picoseconds (time sorted): [{first_cluster_center * 1000:.2f} {second_cluster_center * 1000:.2f}]\")\n",
    "        print(f\"\\nCluster centers in nanoseconds (time sorted): [{first_cluster_center:.8f} {second_cluster_center:.8f}]\")\n",
    "\n",
    "        print(f\"\\n=== Cluster Position Analysis ===\")\n",
    "        print(f\"Position of cluster 0: {first_cluster_center:.6f} ns\")\n",
    "        print(f\"Position of cluster 1: {second_cluster_center:.6f} ns\")\n",
    "        print(f\"Difference between the two clusters: {second_cluster_center - first_cluster_center:.6f} ns\")\n",
    "        \n",
    "        # Also show in picoseconds\n",
    "        print(f\"\\nPosition of cluster 0: {first_cluster_center * 1000:.2f} ps\")\n",
    "        print(f\"Position of cluster 1: {second_cluster_center * 1000:.2f} ps\")\n",
    "        print(f\"Difference between the two clusters: {(second_cluster_center - first_cluster_center) * 1000:.2f} ps\")\n",
    "\n",
    "        # Visualize classification results\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        # Generate colors for clusters\n",
    "        colors = ['#1f77b4', '#ff7f0e']\n",
    "\n",
    "        # Plot cluster centers\n",
    "        plt.scatter(sorted_centers, range(2), color='red', marker='x', s=150, \n",
    "                   linewidths=3, label='Cluster Centers')\n",
    "        \n",
    "        # Plot cluster points\n",
    "        for i in range(2):\n",
    "            cluster_points = mid_positions_mod[cluster_labels == i]\n",
    "            plt.scatter(cluster_points, np.ones(len(cluster_points)) * i, \n",
    "                       color=colors[i % len(colors)], alpha=0.7, s=30, label=f'Cluster {i}')\n",
    "        \n",
    "        # Add vertical line for threshold\n",
    "        plt.axvline(x=THRESHOLD_NS, color='green', linestyle=':', linewidth=2, alpha=0.8, \n",
    "                   label=f'Threshold: {THRESHOLD_NS} ns')\n",
    "        \n",
    "        # Add vertical lines for cluster centers\n",
    "        plt.axvline(x=float(first_cluster_center), color='blue', linestyle='--', linewidth=3, alpha=0.8, \n",
    "                   label=f'Cluster 0 center: {first_cluster_center:.4f} ns')\n",
    "        plt.axvline(x=float(second_cluster_center), color='orange', linestyle='--', linewidth=3, alpha=0.8, \n",
    "                   label=f'Cluster 1 center: {second_cluster_center:.4f} ns')\n",
    "        \n",
    "        # Add text annotations for the cluster centers\n",
    "        plt.text(float(first_cluster_center) + 0.05, 0.7, f'Cluster 0\\nCenter: {first_cluster_center:.4f} ns', \n",
    "                rotation=90, verticalalignment='center', fontsize=10, \n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='blue', alpha=0.1))\n",
    "        plt.text(float(second_cluster_center) + 0.05, 1.3, f'Cluster 1\\nCenter: {second_cluster_center:.4f} ns', \n",
    "                rotation=90, verticalalignment='center', fontsize=10,\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='orange', alpha=0.1))\n",
    "        \n",
    "        plt.xlabel('Mid Position (ns, modulo 2)')\n",
    "        plt.ylabel('Cluster ID')\n",
    "        plt.title(f'Simplified Classification Results (Threshold: {THRESHOLD_NS} ns)')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        print(f\"No mid points found for classification\")\n",
    "        \n",
    "else:\n",
    "    print(\"Please run the mid point detection cell first to generate mid point data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4285c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symbol classification and alignment for 2-cluster PPM demodulation\n",
    "\n",
    "# Symbol classification based on 2 clusters\n",
    "print(f\"\\n=== Symbol Classification ===\")\n",
    "\n",
    "# Direct classification: cluster 0 = bit 0, cluster 1 = bit 1\n",
    "symbol_classifications = cluster_labels\n",
    "\n",
    "# Data reshaping and symbol alignment\n",
    "print(f\"\\n=== Data Reshaping and Symbol Alignment ===\")\n",
    "\n",
    "# Get original signal data and parameters\n",
    "ori_signal = df['data'].values\n",
    "ori_len = len(ori_signal)\n",
    "samples_per_frame = int(config.signal.up_fs / config.signal.fsymbol)\n",
    "\n",
    "# Reshape data into frames\n",
    "trimmed_len = ori_len - (ori_len % samples_per_frame)\n",
    "trimmed_signal = ori_signal[:trimmed_len]\n",
    "frames = trimmed_signal.reshape(-1, samples_per_frame)\n",
    "\n",
    "print(f\"Original signal length: {ori_len}\")\n",
    "print(f\"Samples per symbol: {samples_per_frame}\")\n",
    "print(f\"Trimmed signal length: {trimmed_len}\")\n",
    "print(f\"Frames shape: {frames.shape}\")\n",
    "\n",
    "# Calculate symbol center and offsets\n",
    "symbol_center = samples_per_frame // 2\n",
    "\n",
    "# Convert cluster positions from nanoseconds to sample indices\n",
    "sampling_period = 1 / config.signal.up_fs  # seconds per sample\n",
    "cluster_0_samples = first_cluster_center * 1e-9 / sampling_period  # convert ns to samples\n",
    "cluster_1_samples = second_cluster_center * 1e-9 / sampling_period   # convert ns to samples\n",
    "\n",
    "# Calculate shifts needed to center each cluster\n",
    "x0 = int(round(symbol_center - cluster_0_samples))  # shift for cluster 0 (right shift if positive)\n",
    "x1 = int(round(symbol_center - cluster_1_samples))  # shift for cluster 1 (right shift if positive)\n",
    "# print(x0, x1)  # 214， -170\n",
    "# x0, x1 = 214, -174\n",
    "\n",
    "print(f\"Symbol center position: {symbol_center}\")\n",
    "print(f\"Cluster 0 position (samples): {cluster_0_samples:.2f}\")\n",
    "print(f\"Cluster 1 position (samples): {cluster_1_samples:.2f}\")\n",
    "print(f\"Cluster 0 needs to move: {x0} samples ({'right shift' if x0 > 0 else 'left shift' if x0 < 0 else 'no movement'})\")\n",
    "print(f\"Cluster 1 needs to move: {x1} samples ({'right shift' if x1 > 0 else 'left shift' if x1 < 0 else 'no movement'})\")\n",
    "\n",
    "# Apply shifts to each frame based on classification\n",
    "aligned_frames = np.zeros_like(frames)\n",
    "\n",
    "for i in range(len(frames)):\n",
    "    if i < len(symbol_classifications):  # Ensure we have classification for this frame\n",
    "        if symbol_classifications[i] == 0:  # Cluster 0\n",
    "            shift = x0\n",
    "        else:  # Cluster 1\n",
    "            shift = x1\n",
    "        \n",
    "        # Apply the calculated shift\n",
    "        if shift > 0:  # Right shift\n",
    "            aligned_frames[i, shift:] = frames[i, :-shift]\n",
    "            aligned_frames[i, :shift] = 0  # Fill with zeros\n",
    "        elif shift < 0:  # Left shift\n",
    "            aligned_frames[i, :shift] = frames[i, -shift:]\n",
    "            aligned_frames[i, shift:] = 0  # Fill with zeros\n",
    "        else:  # No shift needed\n",
    "            aligned_frames[i] = frames[i]\n",
    "    else:\n",
    "        aligned_frames[i] = frames[i]  # No classification available, keep original\n",
    "\n",
    "print(f\"Symbol alignment completed, aligned frames shape: {aligned_frames.shape}\")\n",
    "\n",
    "# Visualize the alignment effect\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Show first few symbols before and after alignment\n",
    "num_examples = min(8, len(frames))\n",
    "\n",
    "for i in range(num_examples):\n",
    "    # Original frames\n",
    "    plt.subplot(2, num_examples, i+1)\n",
    "    plt.plot(frames[i], 'b-', alpha=0.7, label='Original')\n",
    "    plt.axvline(x=symbol_center, color='gray', linestyle='--', alpha=0.5, label='Center')\n",
    "    if i < len(symbol_classifications):\n",
    "        cluster_name = f'Cluster {symbol_classifications[i]}'\n",
    "        plt.title(f'Original Symbol {i}\\n({cluster_name})')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    if i == 0:\n",
    "        plt.legend()\n",
    "    \n",
    "    # Aligned frames\n",
    "    plt.subplot(2, num_examples, i+1+num_examples)\n",
    "    plt.plot(aligned_frames[i], 'r-', alpha=0.7, label='Aligned')\n",
    "    plt.axvline(x=symbol_center, color='gray', linestyle='--', alpha=0.5, label='Center')\n",
    "    if i < len(symbol_classifications):\n",
    "        cluster_name = f'Cluster {symbol_classifications[i]}'\n",
    "        shift = x0 if symbol_classifications[i] == 0 else x1\n",
    "        shift_info = f'Move {shift}' if shift != 0 else 'No movement'\n",
    "        plt.title(f'Aligned Symbol {i}\\n({cluster_name}, {shift_info})')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    if i == 0:\n",
    "        plt.legend()\n",
    "\n",
    "plt.suptitle('Symbol Alignment: Before vs After (2-Cluster PPM)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Count symbols in each cluster\n",
    "cluster_0_count = np.sum(symbol_classifications == 0)\n",
    "cluster_1_count = np.sum(symbol_classifications == 1)\n",
    "total_symbols = len(symbol_classifications)\n",
    "\n",
    "print(f\"Total detected symbols: {total_symbols}\")\n",
    "print(f\"Cluster 0: {cluster_0_count} symbols ({cluster_0_count/total_symbols*100:.1f}%)\")\n",
    "print(f\"Cluster 1: {cluster_1_count} symbols ({cluster_1_count/total_symbols*100:.1f}%)\")\n",
    "\n",
    "# Show first 50 classifications\n",
    "print(f\"\\nClassification results for first 50 symbols:\")\n",
    "print(f\"Symbol indices: {list(range(min(50, len(symbol_classifications))))}\")\n",
    "print(f\"Classifications: {symbol_classifications[:50].tolist()}\")\n",
    "\n",
    "# Create a visualization of symbol classifications over time\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plot 1: Symbol classification over time\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(range(len(symbol_classifications)), symbol_classifications, 'o-', markersize=3, alpha=0.7)\n",
    "plt.xlabel('Symbol Index')\n",
    "plt.ylabel('Classification (0=Cluster 0, 1=Cluster 1)')\n",
    "plt.title('2-PPM Symbol Classification Over Time')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yticks([0, 1], ['Cluster 0', 'Cluster 1'])\n",
    "\n",
    "# Plot 2: Histogram of classifications\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.bar(['Cluster 0', 'Cluster 1'], \n",
    "        [cluster_0_count, cluster_1_count], \n",
    "        color=['blue', 'orange'], alpha=0.7)\n",
    "plt.ylabel('Symbol Count')\n",
    "plt.title('Symbol Distribution by Cluster')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add count labels on bars\n",
    "plt.text(0, cluster_0_count + total_symbols*0.01, f'{cluster_0_count}\\n({cluster_0_count/total_symbols*100:.1f}%)', \n",
    "        ha='center', va='bottom')\n",
    "plt.text(1, cluster_1_count + total_symbols*0.01, f'{cluster_1_count}\\n({cluster_1_count/total_symbols*100:.1f}%)', \n",
    "        ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate bit sequence (direct mapping: cluster 0 -> bit 0, cluster 1 -> bit 1)\n",
    "bit_sequence = symbol_classifications\n",
    "print(f\"\\nDemodulated bit sequence (first 100 bits): {bit_sequence[:100].tolist()}\")\n",
    "\n",
    "# Calculate transition statistics\n",
    "transitions = np.diff(symbol_classifications)\n",
    "num_transitions = np.sum(transitions != 0)\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"Total transitions: {num_transitions}\")\n",
    "print(f\"Transition rate: {num_transitions/(len(symbol_classifications)-1)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5252d1e",
   "metadata": {},
   "source": [
    "## IQ demodulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df62921",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_frame = round(config.signal.up_fs / config.signal.fsymbol)\n",
    "\n",
    "t_one_frame = np.linspace(0, 1/config.signal.fsymbol, samples_per_frame, endpoint=False)\n",
    "cosine_wave = np.cos(2 * np.pi * config.signal.fc * t_one_frame)\n",
    "sine_wave = np.sin(2 * np.pi * config.signal.fc * t_one_frame)\n",
    "\n",
    "ori_signal = df['data'].values\n",
    "ori_len = df['data'].size\n",
    "\n",
    "demod_signal = cosine_wave * aligned_frames + sine_wave * aligned_frames * 1j\n",
    "demod_signal = demod_signal.flatten()\n",
    "\n",
    "# Plot the demodulated signal (first 10000 samples)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df['time'][:10000], demod_signal.real[:10000], label='Demodulated Signal (Real Part)', alpha=0.7)\n",
    "plt.plot(df['time'][:10000], demod_signal.imag[:10000], label='Demodulated Signal (Imaginary Part)', alpha=0.7)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Demodulated Signal from 8PPM D2PSK (First 10000 samples)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8399f1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lowpass filter to the demodulated signal\n",
    "from scipy import signal\n",
    "\n",
    "# Filter parameters\n",
    "cutoff_freq = 2e9  # 2 GHz cutoff frequency\n",
    "sampling_rate = config.signal.up_fs  # Use the upsampled frequency\n",
    "nyq = sampling_rate / 2\n",
    "normalized_cutoff = cutoff_freq / nyq\n",
    "\n",
    "# Design Butterworth lowpass filter\n",
    "filter_order = 4\n",
    "b, a = signal.butter(filter_order, normalized_cutoff, btype='low')\n",
    "\n",
    "# Apply zero-phase filtering to both real and imaginary parts\n",
    "demod_signal_filtered = np.zeros_like(demod_signal, dtype=complex)\n",
    "demod_signal_filtered.real = signal.filtfilt(b, a, demod_signal.real)\n",
    "demod_signal_filtered.imag = signal.filtfilt(b, a, demod_signal.imag)\n",
    "\n",
    "print(f\"Applied lowpass filter with cutoff frequency: {cutoff_freq/1e9:.1f} GHz\")\n",
    "print(f\"Filter order: {filter_order}\")\n",
    "print(f\"Sampling rate: {sampling_rate/1e9:.1f} GHz\")\n",
    "print(f\"Normalized cutoff: {normalized_cutoff:.4f}\")\n",
    "\n",
    "# Plot comparison between original and filtered demodulated signal\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plot 1: Real part comparison\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(df['time'][:10000], demod_signal.real[:10000], label='Original Demodulated (Real)', alpha=0.7)\n",
    "plt.plot(df['time'][:10000], demod_signal_filtered.real[:10000], label='Filtered Demodulated (Real)', alpha=0.8)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Demodulated Signal Comparison - Real Part')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Plot 2: Imaginary part comparison\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(df['time'][:10000], demod_signal.imag[:10000], label='Original Demodulated (Imag)', alpha=0.7)\n",
    "plt.plot(df['time'][:10000], demod_signal_filtered.imag[:10000], label='Filtered Demodulated (Imag)', alpha=0.8)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Demodulated Signal Comparison - Imaginary Part')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c62866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize df_upsampled samples belonging to cluster 8\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if clustering results exist\n",
    "if 'cluster_labels' in locals() and 'mid_indices' in locals():\n",
    "    # Find samples belonging to cluster 8\n",
    "    cluster_8_mask = cluster_labels == 7\n",
    "    cluster_8_mid_indices = mid_indices[cluster_8_mask]\n",
    "\n",
    "    cluster_8_mid_indices = [idx // 2048 * 2048 + 1024 for idx in cluster_8_mid_indices]\n",
    "    \n",
    "    print(f\"Found {len(cluster_8_mid_indices)} samples belonging to cluster 8\")\n",
    "    \n",
    "    if len(cluster_8_mid_indices) > 0:\n",
    "        # Get number of samples per symbol\n",
    "        samples_per_frame = int(config.signal.up_fs / config.signal.fsymbol)\n",
    "        print(f\"Samples per frame (symbol): {samples_per_frame}\")\n",
    "        \n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(16, 10))\n",
    "        \n",
    "        # Show first 8 symbols belonging to cluster 8\n",
    "        num_examples = min(8, len(cluster_8_mid_indices))\n",
    "        \n",
    "        for i in range(num_examples):\n",
    "            mid_idx = cluster_8_mid_indices[i]\n",
    "            \n",
    "            # Estimate symbol start and end indices\n",
    "            # Assume mid_idx is near the middle of the symbol\n",
    "            symbol_start = max(0, mid_idx - samples_per_frame // 2)\n",
    "            symbol_end = min(len(df_upsampled), symbol_start + samples_per_frame)\n",
    "            \n",
    "            # Ensure we have a complete symbol\n",
    "            if symbol_end - symbol_start == samples_per_frame:\n",
    "                symbol_data = df_upsampled.iloc[symbol_start:symbol_end]\n",
    "                \n",
    "                plt.subplot(2, 4, i+1)\n",
    "                plt.plot(symbol_data['time'] - symbol_data['time'].iloc[0], symbol_data['data'], 'b-', alpha=0.8)\n",
    "                plt.axvline(x=(mid_idx - symbol_start) * (1/config.signal.up_fs), \n",
    "                           color='red', linestyle='--', alpha=0.7, label='Mid point')\n",
    "                plt.xlabel('Time (s)')\n",
    "                plt.ylabel('Amplitude')\n",
    "                plt.title(f'Cluster 8 Sample {i+1}\\\\nMid idx: {mid_idx}')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                if i == 0:\n",
    "                    plt.legend()\n",
    "        \n",
    "        plt.suptitle('Samples from Cluster 8 in df_upsampled', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Show cluster 8 sample statistics\n",
    "        print(f\"\\\\nCluster 8 Statistics:\")\n",
    "        print(f\"Mid indices: {cluster_8_mid_indices[:10]}...\")  # Show only first 10\n",
    "        print(f\"Mid times (ns): {(df_upsampled.loc[cluster_8_mid_indices[:10], 'time'] * 1e9).tolist()}...\")\n",
    "        \n",
    "        # Calculate average time position of cluster 8 midpoints (mod 2ns)\n",
    "        cluster_8_times = df_upsampled.loc[cluster_8_mid_indices, 'time'].values\n",
    "        cluster_8_times_mod = (cluster_8_times * 1e9) % 2  # Convert to ns and mod 2\n",
    "        avg_time_mod = np.mean(cluster_8_times_mod)\n",
    "        print(f\"Cluster 8 average time position (mod 2ns): {avg_time_mod:.4f} ns\")\n",
    "        \n",
    "        # Overlay display of multiple cluster 8 symbols\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        overlay_count = min(10000, len(cluster_8_mid_indices))\n",
    "        for i in range(overlay_count):\n",
    "            mid_idx = cluster_8_mid_indices[i]\n",
    "            symbol_start = max(0, mid_idx - samples_per_frame // 2)\n",
    "            symbol_end = min(len(df_upsampled), symbol_start + samples_per_frame)\n",
    "            \n",
    "            if symbol_end - symbol_start == samples_per_frame:\n",
    "                symbol_data = df_upsampled.iloc[symbol_start:symbol_end]\n",
    "                time_normalized = symbol_data['time'] - symbol_data['time'].iloc[0]\n",
    "                plt.plot(time_normalized * 1e9, symbol_data['data'], alpha=0.3, color='blue')\n",
    "        \n",
    "        # Calculate average waveform\n",
    "        all_symbols = []\n",
    "        for i in range(len(cluster_8_mid_indices)):\n",
    "            mid_idx = cluster_8_mid_indices[i]\n",
    "            symbol_start = max(0, mid_idx - samples_per_frame // 2)\n",
    "            symbol_end = min(len(df_upsampled), symbol_start + samples_per_frame)\n",
    "            \n",
    "            if symbol_end - symbol_start == samples_per_frame:\n",
    "                symbol_data = df_upsampled.iloc[symbol_start:symbol_end]['data'].values\n",
    "                all_symbols.append(symbol_data)\n",
    "        \n",
    "        if all_symbols:\n",
    "            all_symbols = np.array(all_symbols)\n",
    "            avg_symbol = np.mean(all_symbols, axis=0)\n",
    "            time_axis = np.arange(len(avg_symbol)) * (1/config.signal.up_fs) * 1e9  # in ns\n",
    "            \n",
    "            # plt.plot(time_axis, avg_symbol, 'red', linewidth=3, label=f'Average of {len(all_symbols)} symbols')\n",
    "            plt.axvline(x=time_axis[len(avg_symbol)//2], color='red', linestyle='--', \n",
    "                       alpha=0.7, label='Expected mid point')\n",
    "        \n",
    "        plt.xlabel('Time (ns)')\n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.title(f'Overlay of {overlay_count} symbols from Cluster 8')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        print(\"No samples found for cluster 8\")\n",
    "        \n",
    "else:\n",
    "    print(\"Clustering results not available. Please run K-means clustering analysis first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d3084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "demod_signal_filtered = demod_signal_filtered.reshape(-1, samples_per_frame)\n",
    "\n",
    "# Extract the sampled signal at the specified indices\n",
    "sampled_signal = demod_signal_filtered[:, 1024].flatten()  # Take the 1024th frame and flatten it\n",
    "# Plot the sampled signal\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plot real part\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(range(len(sampled_signal)), sampled_signal.real, 'o-', markersize=4)\n",
    "plt.xlabel('Symbol Index')\n",
    "plt.ylabel('Real Part')\n",
    "plt.title('Sampled Demodulated Signal - Real Part')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot imaginary part\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(range(len(sampled_signal)), sampled_signal.imag, 'o-', markersize=4)\n",
    "plt.xlabel('Symbol Index')\n",
    "plt.ylabel('Imaginary Part')\n",
    "plt.title('Sampled Demodulated Signal - Imaginary Part')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Sampled signal shape: {sampled_signal.shape}\")\n",
    "print(f\"Number of symbols: {len(sampled_signal)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6cfe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional analysis: Cluster distribution in 8-PSK constellation space\n",
    "if 'cluster_labels' in locals() and len(cluster_labels) > 0:\n",
    "    n_points = min(len(sampled_signal), len(cluster_labels))\n",
    "    \n",
    "    # Create a subplot showing cluster statistics\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Cluster phase distribution\n",
    "    ax1.set_title('Cluster Phase Distribution')\n",
    "    cluster_phases = []\n",
    "    cluster_ids = []\n",
    "    \n",
    "    for cluster_id in range(16):\n",
    "        cluster_mask = cluster_labels[:n_points] == cluster_id\n",
    "        cluster_points = sampled_signal[:n_points][cluster_mask]\n",
    "        \n",
    "        if len(cluster_points) > 0:\n",
    "            phases = np.angle(cluster_points, deg=True)\n",
    "            cluster_phases.extend(phases)\n",
    "            cluster_ids.extend([cluster_id] * len(phases))\n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f',\n",
    "              '#bcbd22', '#17becf', '#aec7e8', '#ffbb78', '#98df8a', '#ff9896', '#c5b0d5', '#c49c94']\n",
    "    \n",
    "    for cluster_id in range(16):\n",
    "        cluster_mask = np.array(cluster_ids) == cluster_id\n",
    "        if np.any(cluster_mask):\n",
    "            phases_for_cluster = np.array(cluster_phases)[cluster_mask]\n",
    "            ax1.scatter([cluster_id] * len(phases_for_cluster), phases_for_cluster, \n",
    "                       alpha=0.6, s=20, c=colors[cluster_id % len(colors)], label=f'Cluster {cluster_id}')\n",
    "    \n",
    "    ax1.set_xlabel('Cluster ID')\n",
    "    ax1.set_ylabel('Phase (degrees)')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xticks(range(16))\n",
    "    \n",
    "    # 2. Cluster magnitude distribution\n",
    "    ax2.set_title('Cluster Magnitude Distribution')\n",
    "    cluster_magnitudes = []\n",
    "    cluster_ids_mag = []\n",
    "    \n",
    "    for cluster_id in range(16):\n",
    "        cluster_mask = cluster_labels[:n_points] == cluster_id\n",
    "        cluster_points = sampled_signal[:n_points][cluster_mask]\n",
    "        \n",
    "        if len(cluster_points) > 0:\n",
    "            magnitudes = np.abs(cluster_points)\n",
    "            cluster_magnitudes.extend(magnitudes)\n",
    "            cluster_ids_mag.extend([cluster_id] * len(magnitudes))\n",
    "    \n",
    "    for cluster_id in range(16):\n",
    "        cluster_mask = np.array(cluster_ids_mag) == cluster_id\n",
    "        if np.any(cluster_mask):\n",
    "            mags_for_cluster = np.array(cluster_magnitudes)[cluster_mask]\n",
    "            ax2.scatter([cluster_id] * len(mags_for_cluster), mags_for_cluster, \n",
    "                       alpha=0.6, s=20, c=colors[cluster_id % len(colors)])\n",
    "    \n",
    "    ax2.set_xlabel('Cluster ID')\n",
    "    ax2.set_ylabel('Magnitude')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xticks(range(16))\n",
    "    \n",
    "    # 3. Cluster size distribution\n",
    "    ax3.set_title('Number of Points per Cluster')\n",
    "    cluster_sizes = []\n",
    "    for cluster_id in range(16):\n",
    "        cluster_mask = cluster_labels[:n_points] == cluster_id\n",
    "        cluster_sizes.append(np.sum(cluster_mask))\n",
    "    \n",
    "    bars = ax3.bar(range(16), cluster_sizes, color=[colors[i % len(colors)] for i in range(16)], alpha=0.7)\n",
    "    ax3.set_xlabel('Cluster ID')\n",
    "    ax3.set_ylabel('Number of Points')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_xticks(range(16))\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, size) in enumerate(zip(bars, cluster_sizes)):\n",
    "        if size > 0:\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(cluster_sizes)*0.01,\n",
    "                    str(size), ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # 4. Average phase per cluster (polar plot)\n",
    "    ax4 = plt.subplot(2, 2, 4, projection='polar')\n",
    "    ax4.set_title('Average Phase per Cluster\\\\n(Polar Plot)')\n",
    "    \n",
    "    for cluster_id in range(16):\n",
    "        cluster_mask = cluster_labels[:n_points] == cluster_id\n",
    "        cluster_points = sampled_signal[:n_points][cluster_mask]\n",
    "        \n",
    "        if len(cluster_points) > 0:\n",
    "            avg_phase = np.mean(np.angle(cluster_points))\n",
    "            avg_magnitude = np.mean(np.abs(cluster_points))\n",
    "            ax4.scatter(avg_phase, avg_magnitude, \n",
    "                       s=100, c=colors[cluster_id % len(colors)], \n",
    "                       label=f'Cluster {cluster_id}', alpha=0.8)\n",
    "    \n",
    "    # Add reference 8-PSK points on polar plot\n",
    "    reference_angles = np.linspace(0+np.pi/8, 2*np.pi+np.pi/8, 8, endpoint=False)\n",
    "    reference_radius = np.mean(np.abs(sampled_signal))\n",
    "    ax4.scatter(reference_angles, [reference_radius]*8, \n",
    "               marker='x', s=150, c='red', linewidth=3, label='8-PSK Reference')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(\"\\\\n=== Detailed Cluster Analysis ===\")\n",
    "    print(\"Cluster ID | Points | Avg Phase (°) | Avg Magnitude | Phase Std (°) | Mag Std\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for cluster_id in range(16):\n",
    "        cluster_mask = cluster_labels[:n_points] == cluster_id\n",
    "        cluster_points = sampled_signal[:n_points][cluster_mask]\n",
    "        \n",
    "        if len(cluster_points) > 0:\n",
    "            phases = np.angle(cluster_points, deg=True)\n",
    "            magnitudes = np.abs(cluster_points)\n",
    "            \n",
    "            print(f\"Cluster {cluster_id:2d} | {len(cluster_points):6d} | {np.mean(phases):9.2f} | \"\n",
    "                  f\"{np.mean(magnitudes):11.4f} | {np.std(phases):9.2f} | {np.std(magnitudes):7.4f}\")\n",
    "        else:\n",
    "            print(f\"Cluster {cluster_id:2d} | {0:6d} | {'N/A':>9} | {'N/A':>11} | {'N/A':>9} | {'N/A':>7}\")\n",
    "    \n",
    "    # Calculate which 8-PSK symbol each cluster most likely represents\n",
    "    print(\"\\\\n=== Cluster to 8-PSK Symbol Mapping ===\")\n",
    "    reference_angles_deg = np.degrees(np.linspace(0+np.pi/8, 2*np.pi+np.pi/8, 8, endpoint=False))\n",
    "    \n",
    "    for cluster_id in range(16):\n",
    "        cluster_mask = cluster_labels[:n_points] == cluster_id\n",
    "        cluster_points = sampled_signal[:n_points][cluster_mask]\n",
    "        \n",
    "        if len(cluster_points) > 0:\n",
    "            avg_phase_deg = np.mean(np.angle(cluster_points, deg=True))\n",
    "            # Find closest 8-PSK symbol\n",
    "            phase_differences = np.abs(reference_angles_deg - avg_phase_deg)\n",
    "            closest_symbol = np.argmin(phase_differences)\n",
    "            min_diff = phase_differences[closest_symbol]\n",
    "            \n",
    "            print(f\"Cluster {cluster_id:2d}: Average phase {avg_phase_deg:6.1f}° → \"\n",
    "                  f\"8-PSK Symbol {closest_symbol} (ref: {reference_angles_deg[closest_symbol]:6.1f}°, \"\n",
    "                  f\"diff: {min_diff:5.1f}°)\")\n",
    "    \n",
    "else:\n",
    "    print(\"Cluster labels not available. Please run the K-means clustering analysis first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ccbd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create constellation diagram for the sampled signal\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot constellation points\n",
    "plt.scatter(sampled_signal.real, sampled_signal.imag, alpha=0.7, s=30, c='blue', edgecolors='black', linewidth=0.5)\n",
    "\n",
    "# Add grid and labels\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('In-phase (I)')\n",
    "plt.ylabel('Quadrature (Q)')\n",
    "plt.title('Constellation Diagram of Sampled Signal')\n",
    "plt.axis('equal')\n",
    "\n",
    "# Add circle markers for reference (assuming 8-PSK constellation)\n",
    "# Calculate the radius based on the data\n",
    "radius = np.mean(np.abs(sampled_signal))\n",
    "angles = np.linspace(0+np.pi/8, 2*np.pi+np.pi/8, 8, endpoint=False)\n",
    "reference_points = radius * np.exp(1j * angles)\n",
    "\n",
    "plt.scatter(reference_points.real, reference_points.imag, \n",
    "           marker='x', s=100, c='red', linewidth=2, label='Reference 8-PSK points')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Constellation diagram plotted for {len(sampled_signal)} symbols\")\n",
    "print(f\"Mean magnitude: {np.mean(np.abs(sampled_signal)):.4f}\")\n",
    "print(f\"Standard deviation of magnitude: {np.std(np.abs(sampled_signal)):.4f}\")\n",
    "\n",
    "# Calculate and display statistics\n",
    "magnitudes = np.abs(sampled_signal)\n",
    "phases = np.angle(sampled_signal, deg=True)\n",
    "\n",
    "print(f\"\\nSignal statistics:\")\n",
    "print(f\"Magnitude - Mean: {np.mean(magnitudes):.4f}, Std: {np.std(magnitudes):.4f}\")\n",
    "print(f\"Phase - Mean: {np.mean(phases):.2f}°, Std: {np.std(phases):.2f}°\")\n",
    "print(f\"SNR estimate: {20 * np.log10(np.mean(magnitudes) / np.std(magnitudes)):.2f} dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8988b7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-PSK Demodulation Logic\n",
    "# Map phase angles to corresponding symbols (0-7)\n",
    "\n",
    "def demodulate_8psk(complex_signal, phase_offset):\n",
    "    \"\"\"\n",
    "    Demodulate 8-PSK signal by mapping phase angles to symbols\n",
    "    \n",
    "    Args:\n",
    "        complex_signal: Complex-valued signal samples\n",
    "        phase_offset: Phase offset in radians to align constellation\n",
    "        \n",
    "    Returns:\n",
    "        symbols: Array of demodulated symbols (0-7)\n",
    "    \"\"\"\n",
    "    # Calculate phase angles\n",
    "    phases = np.angle(complex_signal) + phase_offset\n",
    "    \n",
    "    # Normalize phases to [0, 2π)\n",
    "    phases = phases % (2 * np.pi)\n",
    "    \n",
    "    # 8-PSK constellation points (0 to 7)\n",
    "    # Each symbol spans π/4 radians (45 degrees)\n",
    "    symbol_spacing = 2 * np.pi / 8  # π/4\n",
    "    \n",
    "    # Map phases to symbols\n",
    "    symbols = np.round(phases / symbol_spacing).astype(int) % 8\n",
    "    \n",
    "    return symbols\n",
    "\n",
    "# Apply demodulation to the sampled signal\n",
    "demodulated_symbols = demodulate_8psk(sampled_signal, phase_offset=np.pi/8)\n",
    "\n",
    "print(f\"Demodulated {len(demodulated_symbols)} symbols\")\n",
    "print(f\"Symbol range: {demodulated_symbols.min()} to {demodulated_symbols.max()}\")\n",
    "print(f\"First 20 symbols: {demodulated_symbols[:20]}\")\n",
    "\n",
    "# Count symbol occurrences\n",
    "symbol_counts = np.bincount(demodulated_symbols, minlength=8)\n",
    "print(f\"\\nSymbol distribution:\")\n",
    "for i, count in enumerate(symbol_counts):\n",
    "    print(f\"Symbol {i}: {count} occurrences ({count/len(demodulated_symbols)*100:.1f}%)\")\n",
    "\n",
    "# Plot symbol distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(8), symbol_counts)\n",
    "plt.xlabel('Symbol')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Symbol Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(range(8))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(min(100, len(demodulated_symbols))), demodulated_symbols[:100], 'o-', markersize=4)\n",
    "plt.xlabel('Symbol Index')\n",
    "plt.ylabel('Demodulated Symbol')\n",
    "plt.title('First 100 Demodulated Symbols')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yticks(range(8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate phase angles for verification\n",
    "phases_deg = np.angle(sampled_signal, deg=True)\n",
    "print(f\"\\nPhase statistics:\")\n",
    "print(f\"Phase range: {phases_deg.min():.1f}° to {phases_deg.max():.1f}°\")\n",
    "print(f\"Phase mean: {phases_deg.mean():.1f}°\")\n",
    "print(f\"Phase std: {phases_deg.std():.1f}°\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74c83b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Diagnose and fix KeyError: 'data' issue\n",
    "\n",
    "# # Check current DataFrame column names\n",
    "# print(\"df column names:\", df.columns.tolist())\n",
    "# print(\"df_upsampled column names:\", df_upsampled.columns.tolist())\n",
    "\n",
    "# # Fix original code - change 'data' to 'original_data'\n",
    "# original_indices = np.arange(0, len(df_upsampled), upsample_factor)\n",
    "# if len(original_indices) <= len(df):\n",
    "#     # Use correct column name 'original_data' instead of 'data'\n",
    "#     max_diff = np.max(np.abs(df_upsampled.iloc[original_indices]['original_data'].values[:len(df)] - df['original_data'].values))\n",
    "#     print(f\"Maximum difference at original sample points: {max_diff:.2e} (should be ~0)\")\n",
    "# else:\n",
    "#     print(\"Warning: Not enough upsampled points to compare\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
