{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a794668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# Nested configuration structure\n",
    "@dataclass(frozen=True)\n",
    "class SignalConfig:\n",
    "    \"\"\"Signal parameters configuration\"\"\"\n",
    "    fc: float\n",
    "    fsymbol: float\n",
    "    fs: float\n",
    "    up_fs: float\n",
    "    fs_num: int\n",
    "    up_fs_num: int\n",
    "    \n",
    "    # Calculated properties\n",
    "    symbol_duration: float = field(init=False)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        object.__setattr__(self, 'symbol_duration', 1 / self.fsymbol)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class CSVConfig:\n",
    "    \"\"\"CSV file paths configuration\"\"\"\n",
    "    original: str\n",
    "    t0: str\n",
    "    pn_t0: str\n",
    "    fs: str\n",
    "    up_fs: str\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PhaseNoiseConfig:\n",
    "    \"\"\"Phase noise parameters configuration\"\"\"\n",
    "    std_rad: float  # Standard deviation in radians\n",
    "    \n",
    "    # Calculated properties\n",
    "    std_degree: float = field(init=False)\n",
    "    std_time: float = field(init=False)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        object.__setattr__(self, 'std_degree', self.std_rad * 180 / np.pi)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class AWGNConfig:\n",
    "    \"\"\"AWGN parameters configuration\"\"\"\n",
    "    snr_db: float\n",
    "    signal_power: float\n",
    "\n",
    "    # Calculated properties\n",
    "    snr_linear: float = field(init=False)\n",
    "    noise_power: float = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        object.__setattr__(self, 'snr_linear', 10 ** (self.snr_db / 10))\n",
    "        object.__setattr__(self, 'noise_power', self.signal_power / self.snr_linear)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Config:\n",
    "    \"\"\"Main configuration class\"\"\"\n",
    "    signal: SignalConfig\n",
    "    csv: CSVConfig\n",
    "    phase_noise: PhaseNoiseConfig\n",
    "    awgn: AWGNConfig\n",
    "\n",
    "# Create configuration instance\n",
    "config = Config(\n",
    "    signal=SignalConfig(\n",
    "        fc=4e9,\n",
    "        fsymbol=500e6,\n",
    "        fs=32e9,\n",
    "        up_fs=1024e9,\n",
    "        fs_num=32,\n",
    "        up_fs_num=1024\n",
    "    ),\n",
    "    csv=CSVConfig(\n",
    "        original='../csv/8DPSK_500Mbps_5u.csv',\n",
    "        t0='../csv/t0.csv',\n",
    "        pn_t0='../csv/pn_t0.csv',\n",
    "        fs='../csv/fs.csv',\n",
    "        up_fs='../csv/up_fs.csv'\n",
    "    ),\n",
    "    phase_noise=PhaseNoiseConfig(\n",
    "        std_rad=0\n",
    "    ),\n",
    "    awgn=AWGNConfig(\n",
    "        snr_db=16,\n",
    "        signal_power=3e-3\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d271f65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-configure the original file:\n",
    "# 1. change the header to be time,data\n",
    "# 2. make sure the time starts from 0\n",
    "# 3. make the 99% of the data to be 0.99\n",
    "\n",
    "# Read CSV file\n",
    "df = pd.read_csv(config.csv.original)\n",
    "\n",
    "# Get column names and rename to time and data\n",
    "columns = df.columns.tolist()\n",
    "df.columns = ['time', 'data']\n",
    "\n",
    "# Ensure time starts from 0\n",
    "if len(df) > 0:\n",
    "    time_start = df['time'].iloc[0]\n",
    "    df['time'] = df['time'] - time_start\n",
    "\n",
    "# Scale, make the 99% of the data to be 0.99\n",
    "factor = 0.99 / df['data'].quantile(0.99)\n",
    "df['data'] = df['data'] * factor\n",
    "\n",
    "# Save processed file\n",
    "df.to_csv(config.csv.t0, index=False)\n",
    "\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf573aef",
   "metadata": {},
   "source": [
    "## Add phase noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c883ffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(config.csv.t0)\n",
    "\n",
    "# generate phase noise\n",
    "np.random.seed(42)\n",
    "phase_noise = np.random.normal(0, config.phase_noise.std_rad, len(data))\n",
    "\n",
    "# Calculate time jitter noise\n",
    "# Relationship between time jitter and phase noise: Δt = Δφ / (2π * f_carrier)\n",
    "time_jitter = phase_noise / (2 * np.pi * config.signal.fc)\n",
    "\n",
    "# Create output dataframe, keep only time and data columns\n",
    "output_data = pd.DataFrame()\n",
    "\n",
    "# Add time column\n",
    "output_data['time'] = data['time']\n",
    "\n",
    "# Apply time jitter noise to the original signal\n",
    "# Get original time axis\n",
    "t_original = np.array(data['time'].values)\n",
    "t_jittered = t_original + time_jitter\n",
    "data_original = np.array(data['data'].values)\n",
    "\n",
    "# Use interpolation to get signal values at jittered time points\n",
    "# from scipy.interpolate import interp1d\n",
    "\n",
    "# Create interpolation function\n",
    "# Use linear interpolation, extrapolate for boundaries\n",
    "interp_func = sp.interpolate.interp1d(t_original, data_original,\n",
    "                        kind='linear',\n",
    "                        bounds_error=False,\n",
    "                        fill_value=0)\n",
    "\n",
    "# Sample at jittered time points\n",
    "noisy_signal = interp_func(t_jittered)\n",
    "\n",
    "output_data['data'] = noisy_signal\n",
    "\n",
    "# save to csv\n",
    "output_data.to_csv(config.csv.pn_t0, index=False)\n",
    "\n",
    "plot_data = output_data.head(3000)\n",
    "\n",
    "# Read and plot the first 1000 points\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(plot_data['time'] * 1e9, plot_data['data'], label='data')\n",
    "plt.xlabel('Time (ns)')\n",
    "plt.ylabel('Signal value')\n",
    "plt.title('First 1000 points of phase noise/time jitter signal')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b9d830",
   "metadata": {},
   "source": [
    "## downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74541430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the target resampling frequency and interval\n",
    "target_frequency_hz = config.signal.fs\n",
    "resampling_interval_s = 1 / target_frequency_hz\n",
    "\n",
    "# Read the CSV file, skipping the original header row to replace it later\n",
    "df = pd.read_csv(config.csv.t0, header=0)\n",
    "\n",
    "# Rename columns for clarity based on the original header structure\n",
    "# Assuming the first column is time-like and second is data-like\n",
    "df.columns = ['original_time', 'original_data']\n",
    "\n",
    "# Convert columns to numeric, coercing errors if any\n",
    "df['original_time'] = pd.to_numeric(df['original_time'], errors='coerce')\n",
    "df['original_data'] = pd.to_numeric(df['original_data'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values that might have resulted from coercion\n",
    "df.dropna(subset=['original_time', 'original_data'], inplace=True)\n",
    "\n",
    "# Prepare for resampling\n",
    "# The new time axis will start from 0 (because df['original_time'] now starts from 0)\n",
    "# and go up to the maximum duration of the adjusted time\n",
    "start_resample_time = 0\n",
    "end_resample_time = df['original_time'].max()\n",
    "\n",
    "new_time_axis = np.arange(start_resample_time, end_resample_time, resampling_interval_s)\n",
    "\n",
    "# Perform linear interpolation\n",
    "# np.interp needs the original x-values (df['time']) to be sorted\n",
    "# Assert that the data is already sorted by time\n",
    "assert df['original_time'].is_monotonic_increasing, \"Data must be sorted by time\"\n",
    "resampled_data_values = np.interp(new_time_axis, df['original_time'], df['original_data'])\n",
    "\n",
    "# Create a new DataFrame for the resampled data\n",
    "df_resampled = pd.DataFrame({'time': new_time_axis, 'data': resampled_data_values})\n",
    "\n",
    "# Display the resampled data information without saving to file\n",
    "print(f\"Data resampled to {config.signal.fs_num} GHz.\")\n",
    "print(df_resampled.head())\n",
    "\n",
    "# Display comparison of original and resampled signals for the first 30ns\n",
    "time_limit = 30e-9  # 30 ns\n",
    "\n",
    "# Filter data for the first 30ns\n",
    "mask_original = df['original_time'] <= time_limit\n",
    "mask_resampled = df_resampled['time'] <= time_limit\n",
    "\n",
    "# Create comparison plot\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Top plot: Original signal\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(df.loc[mask_original, 'original_time'] * 1e9, \n",
    "        df.loc[mask_original, 'original_data'], \n",
    "        'b-', linewidth=1, alpha=0.8, label='Original Signal')\n",
    "plt.xlabel('Time (ns)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Original Signal - First 30ns')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.xlim(0, 30)\n",
    "\n",
    "# Bottom plot: Signal comparison\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(df.loc[mask_original, 'original_time'] * 1e9, \n",
    "        df.loc[mask_original, 'original_data'], \n",
    "        'b-', linewidth=1, alpha=0.6, label=f'Original Signal')\n",
    "plt.plot(df_resampled.loc[mask_resampled, 'time'] * 1e9, \n",
    "        df_resampled.loc[mask_resampled, 'data'], \n",
    "        'r-', linewidth=1, alpha=0.8, label=f'Resampled Signal ({config.signal.fs_num} GHz)')\n",
    "plt.xlabel('Time (ns)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title(f'Signal Comparison - First 30ns (Original vs {config.signal.fs_num} GHz Resampled)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.xlim(0, 30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(f\"\\n=== First 30ns Signal Statistics ===\")\n",
    "print(f\"Original signal points: {mask_original.sum()}\")\n",
    "print(f\"Resampled signal points: {mask_resampled.sum()}\")\n",
    "print(f\"Original sampling rate: {1/df['original_time'].diff().mean()/1e9:.2f} GHz (estimated)\")\n",
    "print(f\"Resampled rate: {target_frequency_hz/1e9:.2f} GHz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06794ce",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe27f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the square of the data to find power\n",
    "df_resampled['Data_Squared'] = df_resampled['data'] ** 2\n",
    "\n",
    "# Define the period for folding (assuming 500 Mbps symbol rate -> 2 ns period)\n",
    "period = 2e-9  # 2 ns\n",
    "\n",
    "# Use the squared data for folding analysis\n",
    "data_to_fold = df_resampled['Data_Squared'].dropna()\n",
    "time_to_fold = df_resampled.loc[data_to_fold.index, 'time']\n",
    "\n",
    "# Calculate the time modulo the period\n",
    "folded_time = time_to_fold % period\n",
    "\n",
    "# Determine the time resolution\n",
    "time_resolution = time_to_fold.diff().mean()\n",
    "if pd.isna(time_resolution):\n",
    "    time_resolution = (df_resampled['time'].iloc[1] - df_resampled['time'].iloc[0]) if len(df_resampled['time']) > 1 else 1e-12\n",
    "\n",
    "# Create bins for the 0-2ns range\n",
    "num_bins = max(1, int(period / time_resolution))\n",
    "bins = np.linspace(0, period, num_bins + 1)\n",
    "\n",
    "# Create a dataframe for folding\n",
    "fold_df = pd.DataFrame({'time': folded_time, 'data': data_to_fold})\n",
    "\n",
    "# Digitize the folded time to assign each time point to a bin\n",
    "fold_df['time_bin'] = pd.cut(fold_df['time'], bins=bins, labels=False, include_lowest=True)\n",
    "\n",
    "# Group by the bins and sum the data\n",
    "summed_data = fold_df.groupby('time_bin')['data'].sum()\n",
    "\n",
    "# Create the time axis for the summed data (using the middle of each bin)\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "# Reindex the summed data to match the bins, filling missing bins with 0\n",
    "summed_data = summed_data.reindex(range(len(bin_centers)), fill_value=0)\n",
    "\n",
    "# Normalize the summed data to be between 0 and 1\n",
    "summed_data = (summed_data - summed_data.min()) / (summed_data.max() - summed_data.min())\n",
    "\n",
    "# Plot the folded result\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(bin_centers, summed_data)\n",
    "plt.xlabel('Time (s) within 2ns period')\n",
    "plt.ylabel('Summed Data (Normalized)')\n",
    "plt.title('Data folded and summed over a 2ns period')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find the midpoint of the interval where data is above a threshold\n",
    "threshold = 0.2\n",
    "\n",
    "# Find where data is greater than threshold\n",
    "above_threshold = summed_data > threshold\n",
    "\n",
    "if above_threshold.any():\n",
    "    # Get the time values for these points\n",
    "    time_above_threshold = bin_centers[above_threshold]\n",
    "\n",
    "    # Calculate the circular mean of the time points to handle wrap-around\n",
    "    # Convert time to angles (radians)\n",
    "    angles = (time_above_threshold / period) * 2 * np.pi\n",
    "\n",
    "    # Compute the mean of the sines and cosines of the angles\n",
    "    mean_sin = np.mean(np.sin(angles))\n",
    "    mean_cos = np.mean(np.cos(angles))\n",
    "\n",
    "    # Calculate the mean angle from the mean sine and cosine\n",
    "    mean_angle = np.arctan2(mean_sin, mean_cos)\n",
    "\n",
    "    # Convert the mean angle back to time\n",
    "    midpoint_time = (mean_angle / (2 * np.pi)) * period\n",
    "\n",
    "    # Adjust the midpoint to be in the [0, period] range\n",
    "    if midpoint_time < 0:\n",
    "        midpoint_time += period\n",
    "\n",
    "    print(f\"The midpoint of the interval where data is > {threshold} is: {midpoint_time:.4e} s\")\n",
    "    print(f\"Need to delay the signal by {period - midpoint_time:.4e} s to align with the end of period.\")\n",
    "    print(f\"This corresponds to {int((period - midpoint_time) * config.signal.fs)} samples at {config.signal.fs_num} GHz sampling rate.\")\n",
    "\n",
    "    # Plot the result with midpoint visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(bin_centers, summed_data, label='Summed Data')\n",
    "    plt.axhline(y=threshold, color='r', linestyle='--', label=f'Threshold ({threshold})')\n",
    "    plt.axvline(x=midpoint_time, color='g', linestyle='-', label=f'Midpoint ({midpoint_time:.2e} s)')\n",
    "    plt.xlabel('Time (s) within 2ns period')\n",
    "    plt.ylabel('Summed Data (Normalized)')\n",
    "    plt.title('Data folded and summed over a 2ns period with Midpoint')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(f\"No data points found above the threshold of {threshold}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dff7e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first (midpoint_time + 1e-9) seconds of data\n",
    "# Calculate the time threshold\n",
    "time_threshold = midpoint_time + 1e-9  # Add 1 ns to midpoint_time\n",
    "\n",
    "# Find the index where time first exceeds the threshold\n",
    "indices_to_remove = df_resampled['time'] < time_threshold\n",
    "num_points_to_remove = indices_to_remove.sum()\n",
    "\n",
    "print(f\"Time threshold: {time_threshold:.4e} s\")\n",
    "print(f\"Number of data points to remove: {num_points_to_remove}\")\n",
    "\n",
    "# Create the trimmed dataframe\n",
    "df_trimmed = df_resampled[~indices_to_remove].copy()\n",
    "\n",
    "# Reset the time axis to start from 0 again\n",
    "df_trimmed['time'] = df_trimmed['time'] - df_trimmed['time'].min()\n",
    "\n",
    "# Reset the index\n",
    "df_trimmed.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Original data shape: {df_resampled.shape}\")\n",
    "print(f\"Trimmed data shape: {df_trimmed.shape}\")\n",
    "print(f\"Data points removed: {df_resampled.shape[0] - df_trimmed.shape[0]}\")\n",
    "print(f\"First few rows of trimmed data:\")\n",
    "print(df_trimmed.head())\n",
    "\n",
    "# Plot the trimmed data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_trimmed['time'], df_trimmed['data'])\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Data')\n",
    "plt.title('Trimmed Data vs Time')\n",
    "plt.grid(True)\n",
    "plt.xlim(0, 1e-8)  # Show 0-10 ns\n",
    "# Set x-axis ticks every 2 ns\n",
    "plt.xticks(np.arange(0, 1e-8 + 2e-9, 2e-9))\n",
    "plt.show()\n",
    "\n",
    "# Save the trimmed data to CSV file\n",
    "# Only save time and data columns (exclude Data_Squared)\n",
    "df_to_save = pd.DataFrame(df_trimmed[['time', 'data']])\n",
    "df_to_save.to_csv(config.csv.fs, index=False)\n",
    "print(f\"Trimmed data saved to {config.csv.fs}\")\n",
    "print(f\"Saved data shape: {df_to_save.shape}\")\n",
    "print(f\"Columns saved: {list(df_to_save.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92c3331",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df_trimmed[['time', 'data']])\n",
    "\n",
    "# Generate AWGN noise\n",
    "noise = np.random.normal(loc=0, scale=np.sqrt(config.awgn.noise_power), size=len(df))\n",
    "\n",
    "# Add noise to the signal\n",
    "df['data'] = df['data']  # + noise\n",
    "\n",
    "print(f\"Added AWGN noise: SNR={config.awgn.snr_db} dB (linear={config.awgn.snr_linear:.2f}), signal power={config.awgn.signal_power}, noise power={config.awgn.noise_power:.2e}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3122c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: FFT-based zero-padding upsampling from fs_num GHz to 2048 GHz\n",
    "print(f\"=== FFT Zero-Padding Upsampling: {config.signal.fs_num} GHz → {config.signal.up_fs_num} GHz ===\")\n",
    "\n",
    "# Use the loaded data from the CSV file\n",
    "x_orig = df['data'].to_numpy(dtype=np.float64)\n",
    "N = len(x_orig)\n",
    "upsample_factor = int(config.signal.up_fs_num / config.signal.fs_num)\n",
    "print(f\"Original data length: {N}\")\n",
    "print(f\"Original sampling rate: {config.signal.fs_num} GHz\")\n",
    "print(f\"Target sampling rate: {config.signal.up_fs_num} GHz\")\n",
    "print(f\"Upsampling factor: {upsample_factor}×\")\n",
    "\n",
    "# Step 1: FFT of original data\n",
    "X = np.fft.fft(x_orig)\n",
    "\n",
    "# Step 2: Create zero-padded frequency domain signal\n",
    "N_new = N * upsample_factor\n",
    "X_padded = np.zeros(N_new, dtype=complex)\n",
    "\n",
    "# For even N: split the Nyquist frequency component\n",
    "if N % 2 == 0:\n",
    "    # Copy positive frequencies [0, N/2]\n",
    "    X_padded[:N//2] = X[:N//2]\n",
    "    # Copy negative frequencies [N/2+1, N-1] to the end\n",
    "    X_padded[N_new-N//2+1:] = X[N//2+1:]\n",
    "    # Split Nyquist frequency (if it exists)\n",
    "    X_padded[N//2] = X[N//2] / 2\n",
    "    X_padded[N_new-N//2] = X[N//2] / 2\n",
    "else:\n",
    "    # For odd N: simpler case\n",
    "    X_padded[:(N+1)//2] = X[:(N+1)//2]\n",
    "    X_padded[N_new-(N-1)//2:] = X[(N+1)//2:]\n",
    "\n",
    "# Step 3: IFFT to get upsampled signal\n",
    "x_upsampled = np.fft.ifft(X_padded).real * upsample_factor  # Scale by upsampling factor\n",
    "\n",
    "# Step 4: Create precise time axis\n",
    "Ts_orig = 1 / config.signal.fs  # Original sampling period\n",
    "Ts_upsampled = Ts_orig / upsample_factor  # New sampling period\n",
    "t_upsampled = np.arange(len(x_upsampled), dtype=np.float64) * Ts_upsampled\n",
    "\n",
    "print(f\"Upsampled data length: {len(x_upsampled)}\")\n",
    "print(f\"New sampling period: {Ts_upsampled:.2e} s\")\n",
    "print(f\"New sampling rate: {1/Ts_upsampled/1e9:.1f} GHz\")\n",
    "\n",
    "# Create DataFrame for the upsampled data\n",
    "df_upsampled = pd.DataFrame({\n",
    "    'time': t_upsampled,\n",
    "    'data': x_upsampled\n",
    "})\n",
    "\n",
    "print(f\"Upsampled data shape: {df_upsampled.shape}\")\n",
    "print(f\"First few rows of {config.signal.up_fs_num} GHz data:\")\n",
    "print(df_upsampled.head())\n",
    "\n",
    "# Verify the upsampling quality by checking the time axis\n",
    "print(f\"\\nTime axis verification:\")\n",
    "print(f\"Original max time: {df['time'].max():.2e} s\")\n",
    "print(f\"Upsampled max time: {t_upsampled.max():.2e} s\")\n",
    "print(f\"Time axis ratio: {t_upsampled.max() / df['time'].max():.6f} (should be close to 1.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d920fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison between original and upsampled data\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot 1: First 1e-8 seconds comparison\n",
    "plt.subplot(2, 1, 1)\n",
    "time_limit = 1e-8\n",
    "\n",
    "# fs_num GHz data\n",
    "mask_orig = df['time'] <= time_limit\n",
    "plt.plot(df.loc[mask_orig, 'time'], df.loc[mask_orig, 'data'], \n",
    "         'o-', markersize=3, linewidth=1, label=f'{config.signal.fs_num} GHz (original)', alpha=0.7)\n",
    "\n",
    "# up_fs_num GHz data\n",
    "mask_upsampled = df_upsampled['time'] <= time_limit\n",
    "plt.plot(df_upsampled.loc[mask_upsampled, 'time'], df_upsampled.loc[mask_upsampled, 'data'], \n",
    "         '-', linewidth=0.8, label=f'{config.signal.up_fs_num} GHz (upsampled)', alpha=0.9)\n",
    "\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Data')\n",
    "plt.title(f'Comparison: {config.signal.fs_num} GHz vs {config.signal.up_fs_num} GHz Data (First 10 ns)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.xlim(0, time_limit)\n",
    "plt.xticks(np.arange(0, time_limit + 2e-9, 2e-9))\n",
    "\n",
    "# Plot 2: Zoomed view of one pulse\n",
    "plt.subplot(2, 1, 2)\n",
    "time_start = 0\n",
    "time_end = 2e-9  # First 2 ns only\n",
    "\n",
    "mask_orig_zoom = (df['time'] >= time_start) & (df['time'] <= time_end)\n",
    "mask_upsampled_zoom = (df_upsampled['time'] >= time_start) & (df_upsampled['time'] <= time_end)\n",
    "\n",
    "plt.plot(df.loc[mask_orig_zoom, 'time'], df.loc[mask_orig_zoom, 'data'], \n",
    "         'o-', markersize=4, linewidth=1.5, label=f'{config.signal.fs_num} GHz (original)')\n",
    "\n",
    "plt.plot(df_upsampled.loc[mask_upsampled_zoom, 'time'], df_upsampled.loc[mask_upsampled_zoom, 'data'], \n",
    "         '-', linewidth=1, label=f'{config.signal.up_fs_num} GHz (upsampled)')\n",
    "\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Data')\n",
    "plt.title('Zoomed View: Single Pulse (First 2 ns)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.xlim(time_start, time_end)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the up_fs_num GHz data to CSV\n",
    "df_upsampled.to_csv(config.csv.up_fs, index=False)\n",
    "print(f\"\\n{config.signal.up_fs_num} GHz upsampled data saved to: {config.csv.up_fs}\")\n",
    "print(f\"File size: ~{len(df_upsampled) * 2 * 8 / 1024**2:.1f} MB\")\n",
    "\n",
    "# Calculate some quality metrics\n",
    "print(f\"\\nUpsampling quality check:\")\n",
    "print(f\"Original data points: {len(df)}\")\n",
    "print(f\"Upsampled data points: {len(df_upsampled)} (should be {len(df) * upsample_factor})\")\n",
    "print(f\"Ratio: {len(df_upsampled) / len(df):.1f} (should be {upsample_factor:.1f})\")\n",
    "\n",
    "# Check if the upsampled data preserves the original samples\n",
    "original_indices = np.arange(0, len(df_upsampled), upsample_factor)\n",
    "if len(original_indices) <= len(df):\n",
    "    max_diff = np.max(np.abs(df_upsampled.iloc[original_indices]['data'].values[:len(df)] - df['data'].values))\n",
    "    print(f\"Maximum difference at original sample points: {max_diff:.2e} (should be ~0)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4349ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(config.csv.up_fs)\n",
    "\n",
    "print(f\"Reading file: {config.csv.up_fs}\")  # Add this line to confirm which file is being read\n",
    "\n",
    "# Plot Data column vs Time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['time'], df['data'])\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Data')\n",
    "plt.title('Data vs Time')\n",
    "plt.grid(True)\n",
    "plt.xlim(0, 1e-8)\n",
    "plt.show()\n",
    "\n",
    "# Calculate the square of the data\n",
    "df['Data_Squared'] = df['data'] ** 2\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['time'], df['Data_Squared'])\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Data Squared')\n",
    "plt.title('Data Squared vs Time')\n",
    "plt.grid(True)\n",
    "plt.xlim(0, 2e-8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adbebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Butterworth low-pass filter parameters\n",
    "native_sampling_rate = config.signal.up_fs  # Use the upsampled frequency\n",
    "cutoff = 2e9  # 4 GHz cutoff frequency\n",
    "N = 4  # Filter order\n",
    "nyq = native_sampling_rate / 2\n",
    "cutoff_norm = cutoff / nyq\n",
    "\n",
    "# Design Butterworth filter\n",
    "b, a = sp.signal.butter(N, cutoff_norm, btype='low')\n",
    "\n",
    "# Apply zero-phase filtering\n",
    "df['Data_MA'] = sp.signal.filtfilt(b, a, df['Data_Squared'])\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.plot(df['time'], df['Data_Squared'], label='Data Squared', alpha=0.7)\n",
    "plt.plot(df['time'], df['Data_MA'], label=f'Butterworth LPF (N={N})', linewidth=2)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Butterworth Low-pass Filter Effect')\n",
    "plt.legend()\n",
    "plt.xlim(0, 4e-8)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5fec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set threshold\n",
    "threshold = 0.1\n",
    "native_sampling_rate = config.signal.up_fs  # Use the upsampled frequency\n",
    "symbol_rate = config.signal.fsymbol  # 500 MHz\n",
    "\n",
    "# Get data\n",
    "filtered_data = df[\"Data_MA\"].values\n",
    "\n",
    "# Calculate only falling edges\n",
    "above = filtered_data > threshold\n",
    "falling_edges = np.where(np.diff(above.astype(int)) == -1)[0] + 1  # Falling edges\n",
    "\n",
    "# Calculate midpoint by shifting 512 samples to the left from falling edges\n",
    "if falling_edges.size > 0:\n",
    "    # Shift 512 samples to the left from each falling edge\n",
    "    mid_indices = falling_edges - 512\n",
    "    \n",
    "    # Ensure indices are within valid range (non-negative)\n",
    "    mid_indices = mid_indices[mid_indices >= 0]\n",
    "    \n",
    "    mid_times = df.loc[mid_indices, 'time'].values\n",
    "\n",
    "    print(f\"Found {len(falling_edges)} falling edges\")\n",
    "    print(f\"Found {len(mid_indices)} valid midpoints (512 samples left of falling edges)\")\n",
    "    print(f\"Midpoint times: {mid_times}\")\n",
    "else:\n",
    "    mid_indices = np.array([])\n",
    "    mid_times = np.array([])\n",
    "    print(\"No falling edges detected\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df['time'], df['Data_MA'], label='Filtered Data (Data_MA)')\n",
    "if falling_edges.size > 0:\n",
    "    plt.plot(df.loc[falling_edges, 'time'], filtered_data[falling_edges], 'rv', label='Falling Edge')\n",
    "if mid_indices.size > 0:\n",
    "    plt.plot(df.loc[mid_indices, 'time'], filtered_data[mid_indices], 'ko', label='Mid Points (512 samples left)')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Filtered Data (Moving Average)')\n",
    "plt.title('Falling Edges and Mid Points (512 samples left shift)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.xlim(0, 2e-8)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f68f92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all mid positions and use K-means clustering\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Configuration parameters\n",
    "N_CLUSTERS = 1  # Number of clusters for K-means analysis\n",
    "\n",
    "# Extract mid positions from the previous analysis\n",
    "if 'mid_indices' in locals() and 'mid_times' in locals():\n",
    "    # List of mid indices in the dataframe\n",
    "    mid_indices_list = mid_indices.tolist()\n",
    "    \n",
    "    # List of mid time positions\n",
    "    mid_positions = mid_times.tolist()\n",
    "    \n",
    "    print(\"Mid Point Analysis Results:\")\n",
    "    print(f\"Number of mid points found: {len(mid_positions)}\")\n",
    "    print(f\"Mid indices: {mid_indices_list}\")\n",
    "    print(f\"Mid time positions (seconds): {mid_positions}\")\n",
    "    \n",
    "    # Convert to more readable format (nanoseconds)\n",
    "    mid_positions_ns = [pos * 1e9 for pos in mid_positions]\n",
    "    print(f\"Mid time positions (nanoseconds): {mid_positions_ns}\")\n",
    "    \n",
    "    # Calculate relative positions from the first mid point\n",
    "    if len(mid_positions) > 1:\n",
    "        relative_positions = [(pos - mid_positions[0]) * 1e9 for pos in mid_positions]\n",
    "        print(f\"Relative positions from first mid point (ns): {relative_positions}\")\n",
    "        \n",
    "        # Calculate intervals between consecutive mid points\n",
    "        intervals_ns = [pos * 1e9 for pos in np.diff(mid_positions)]\n",
    "        print(f\"Intervals between consecutive mid points (ns): {intervals_ns}\")\n",
    "        \n",
    "    # Use K-means clustering on mid positions (modulo 2ns for periodic analysis)\n",
    "    if len(mid_positions_ns) > N_CLUSTERS:  # Need at least N_CLUSTERS points for clustering\n",
    "        mid_positions_mod = np.array([pos % 2 for pos in mid_positions_ns]).reshape(-1, 1)\n",
    "        \n",
    "        # Apply K-means with N_CLUSTERS clusters\n",
    "        kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=42)\n",
    "        cluster_labels_original = kmeans.fit_predict(mid_positions_mod)\n",
    "        cluster_centers = kmeans.cluster_centers_.flatten()\n",
    "        \n",
    "        # Create mapping to sort clusters by time position\n",
    "        center_time_pairs = [(i, center) for i, center in enumerate(cluster_centers)]\n",
    "        center_time_pairs.sort(key=lambda x: x[1])  # Sort by time position\n",
    "        \n",
    "        # Create mapping from original cluster ID to time-sorted cluster ID\n",
    "        old_to_new_mapping = {}\n",
    "        for new_id, (old_id, _) in enumerate(center_time_pairs):\n",
    "            old_to_new_mapping[old_id] = new_id\n",
    "        \n",
    "        # Apply the mapping to cluster labels\n",
    "        cluster_labels = np.array([old_to_new_mapping[label] for label in cluster_labels_original])\n",
    "        \n",
    "        # Sort cluster centers by time position\n",
    "        sorted_centers = [pair[1] for pair in center_time_pairs]\n",
    "        \n",
    "        print(f\"\\n=== K-means Clustering Results ({N_CLUSTERS} clusters) - Time Sorted ===\")\n",
    "        for i, center in enumerate(sorted_centers):\n",
    "            cluster_size = np.sum(cluster_labels == i)\n",
    "            print(f\"Cluster {i}: Time = {center:.4f} ns (modulo 2), Points = {cluster_size}\")\n",
    "        \n",
    "        # Convert cluster centers to picoseconds and display as list\n",
    "        # cluster_centers_ps = [int(round(center * 1000)) for center in sorted_centers]\n",
    "        # print(f\"\\nCluster centers in picoseconds (time sorted): {cluster_centers_ps}\")\n",
    "        print(f\"\\nCluster centers in picoseconds (time sorted): [\", end=\"\")\n",
    "        for i in range(len(sorted_centers)):\n",
    "            print(f\"{sorted_centers[i] * 1000:.2f}\", end=\" \" if i < len(sorted_centers) - 1 else \"\")\n",
    "        print(\"]\")\n",
    "\n",
    "        # Print a nanosecond list\n",
    "        print(f\"\\nCluster centers in nanoseconds (time sorted): [\", end=\"\")\n",
    "        for i in range(len(sorted_centers)):\n",
    "            print(f\"{sorted_centers[i]:.8f}\", end=\" \" if i < len(sorted_centers) - 1 else \"\")\n",
    "        print(\"]\")\n",
    "\n",
    "        # Visualize clustering results\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        # Generate enough colors for all clusters\n",
    "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f',\n",
    "                  '#bcbd22', '#17becf', '#aec7e8', '#ffbb78', '#98df8a', '#ff9896', '#c5b0d5', '#c49c94']\n",
    "        # Ensure we have enough colors\n",
    "        while len(colors) < N_CLUSTERS:\n",
    "            colors.extend(colors)\n",
    "\n",
    "        # Plot cluster centers (now time-sorted)\n",
    "        plt.scatter(sorted_centers, range(N_CLUSTERS), color='red', marker='x', s=150, \n",
    "                   linewidths=3, label='Cluster Centers')\n",
    "        \n",
    "        # Plot cluster points\n",
    "        for i in range(N_CLUSTERS):\n",
    "            cluster_points = mid_positions_mod[cluster_labels == i]\n",
    "            plt.scatter(cluster_points, np.ones(len(cluster_points)) * i, \n",
    "                       color=colors[i % len(colors)], alpha=0.7, s=30, label=f'Cluster {i}')\n",
    "        \n",
    "        plt.xlabel('Mid Position (ns, modulo 2)')\n",
    "        plt.ylabel('Cluster ID (Time Sorted)')\n",
    "        plt.title(f'K-means Clustering of Mid Positions ({N_CLUSTERS} clusters, Time Sorted)')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        print(f\"Not enough mid points for K-means clustering (found {len(mid_positions_ns)}, need > {N_CLUSTERS})\")\n",
    "        \n",
    "else:\n",
    "    print(\"Please run the mid point detection cell first to generate mid point data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5252d1e",
   "metadata": {},
   "source": [
    "## IQ demodulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df62921",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_frame = round(config.signal.up_fs / config.signal.fsymbol)\n",
    "\n",
    "t_one_frame = np.linspace(0, 1/config.signal.fsymbol, samples_per_frame, endpoint=False)\n",
    "cosine_wave = np.cos(2 * np.pi * config.signal.fc * t_one_frame)\n",
    "sine_wave = np.sin(2 * np.pi * config.signal.fc * t_one_frame)\n",
    "\n",
    "ori_signal = df['data'].values\n",
    "ori_len = df['data'].size\n",
    "trimmed_len = ori_len - (ori_len % samples_per_frame)\n",
    "trimmed_signal = ori_signal[:trimmed_len]\n",
    "# Reshape the signal into frames\n",
    "frames = trimmed_signal.reshape(-1, samples_per_frame)\n",
    "\n",
    "demod_signal = cosine_wave * frames + sine_wave * frames * 1j\n",
    "demod_signal = demod_signal.flatten()\n",
    "\n",
    "# Plot the demodulated signal (first 10000 samples)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df['time'][:10000], demod_signal.real[:10000], label='Demodulated Signal (Real Part)', alpha=0.7)\n",
    "plt.plot(df['time'][:10000], demod_signal.imag[:10000], label='Demodulated Signal (Imaginary Part)', alpha=0.7)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Demodulated Signal from 8PPM D2PSK (First 10000 samples)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8399f1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lowpass filter to the demodulated signal\n",
    "from scipy import signal\n",
    "\n",
    "# Filter parameters\n",
    "cutoff_freq = 2e9  # 2 GHz cutoff frequency\n",
    "sampling_rate = config.signal.up_fs  # Use the upsampled frequency\n",
    "nyq = sampling_rate / 2\n",
    "normalized_cutoff = cutoff_freq / nyq\n",
    "\n",
    "# Design Butterworth lowpass filter\n",
    "filter_order = 4\n",
    "b, a = signal.butter(filter_order, normalized_cutoff, btype='low')\n",
    "\n",
    "# Apply zero-phase filtering to both real and imaginary parts\n",
    "demod_signal_filtered = np.zeros_like(demod_signal, dtype=complex)\n",
    "demod_signal_filtered.real = signal.filtfilt(b, a, demod_signal.real)\n",
    "demod_signal_filtered.imag = signal.filtfilt(b, a, demod_signal.imag)\n",
    "\n",
    "print(f\"Applied lowpass filter with cutoff frequency: {cutoff_freq/1e9:.1f} GHz\")\n",
    "print(f\"Filter order: {filter_order}\")\n",
    "print(f\"Sampling rate: {sampling_rate/1e9:.1f} GHz\")\n",
    "print(f\"Normalized cutoff: {normalized_cutoff:.4f}\")\n",
    "\n",
    "# Plot comparison between original and filtered demodulated signal\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plot 1: Real part comparison\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(df['time'][:10000], demod_signal.real[:10000], label='Original Demodulated (Real)', alpha=0.7)\n",
    "plt.plot(df['time'][:10000], demod_signal_filtered.real[:10000], label='Filtered Demodulated (Real)', alpha=0.8)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Demodulated Signal Comparison - Real Part')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Plot 2: Imaginary part comparison\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(df['time'][:10000], demod_signal.imag[:10000], label='Original Demodulated (Imag)', alpha=0.7)\n",
    "plt.plot(df['time'][:10000], demod_signal_filtered.imag[:10000], label='Filtered Demodulated (Imag)', alpha=0.8)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Demodulated Signal Comparison - Imaginary Part')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d3084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "demod_signal_filtered = demod_signal_filtered.reshape(-1, samples_per_frame)\n",
    "\n",
    "# Extract the sampled signal at the specified indices\n",
    "sampled_signal = demod_signal_filtered[:, 1024].flatten()  # Take the 1024th frame and flatten it\n",
    "# Plot the sampled signal\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plot real part\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(range(len(sampled_signal)), sampled_signal.real, 'o-', markersize=4)\n",
    "plt.xlabel('Symbol Index')\n",
    "plt.ylabel('Real Part')\n",
    "plt.title('Sampled Demodulated Signal - Real Part')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot imaginary part\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(range(len(sampled_signal)), sampled_signal.imag, 'o-', markersize=4)\n",
    "plt.xlabel('Symbol Index')\n",
    "plt.ylabel('Imaginary Part')\n",
    "plt.title('Sampled Demodulated Signal - Imaginary Part')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Sampled signal shape: {sampled_signal.shape}\")\n",
    "print(f\"Number of symbols: {len(sampled_signal)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ccbd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create constellation diagram for the sampled signal\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot constellation points\n",
    "plt.scatter(sampled_signal.real, sampled_signal.imag, alpha=0.7, s=30, c='blue', edgecolors='black', linewidth=0.5)\n",
    "\n",
    "# Add grid and labels\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('In-phase (I)')\n",
    "plt.ylabel('Quadrature (Q)')\n",
    "plt.title('Constellation Diagram of Sampled Signal')\n",
    "plt.axis('equal')\n",
    "\n",
    "# Add circle markers for reference (assuming 8-PSK constellation)\n",
    "# Calculate the radius based on the data\n",
    "radius = np.mean(np.abs(sampled_signal))\n",
    "angles = np.linspace(0+np.pi/8, 2*np.pi+np.pi/8, 8, endpoint=False)\n",
    "reference_points = radius * np.exp(1j * angles)\n",
    "\n",
    "plt.scatter(reference_points.real, reference_points.imag, \n",
    "           marker='x', s=100, c='red', linewidth=2, label='Reference 8-PSK points')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Constellation diagram plotted for {len(sampled_signal)} symbols\")\n",
    "print(f\"Mean magnitude: {np.mean(np.abs(sampled_signal)):.4f}\")\n",
    "print(f\"Standard deviation of magnitude: {np.std(np.abs(sampled_signal)):.4f}\")\n",
    "\n",
    "# Calculate and display statistics\n",
    "magnitudes = np.abs(sampled_signal)\n",
    "phases = np.angle(sampled_signal, deg=True)\n",
    "\n",
    "print(f\"\\nSignal statistics:\")\n",
    "print(f\"Magnitude - Mean: {np.mean(magnitudes):.4f}, Std: {np.std(magnitudes):.4f}\")\n",
    "print(f\"Phase - Mean: {np.mean(phases):.2f}°, Std: {np.std(phases):.2f}°\")\n",
    "print(f\"SNR estimate: {20 * np.log10(np.mean(magnitudes) / np.std(magnitudes)):.2f} dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8988b7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-PSK Demodulation Logic\n",
    "# Map phase angles to corresponding symbols (0-7)\n",
    "\n",
    "def demodulate_8psk(complex_signal, phase_offset):\n",
    "    \"\"\"\n",
    "    Demodulate 8-PSK signal by mapping phase angles to symbols\n",
    "    \n",
    "    Args:\n",
    "        complex_signal: Complex-valued signal samples\n",
    "        phase_offset: Phase offset in radians to align constellation\n",
    "        \n",
    "    Returns:\n",
    "        symbols: Array of demodulated symbols (0-7)\n",
    "    \"\"\"\n",
    "    # Calculate phase angles\n",
    "    phases = np.angle(complex_signal) + phase_offset\n",
    "    \n",
    "    # Normalize phases to [0, 2π)\n",
    "    phases = phases % (2 * np.pi)\n",
    "    \n",
    "    # 8-PSK constellation points (0 to 7)\n",
    "    # Each symbol spans π/4 radians (45 degrees)\n",
    "    symbol_spacing = 2 * np.pi / 8  # π/4\n",
    "    \n",
    "    # Map phases to symbols\n",
    "    symbols = np.round(phases / symbol_spacing).astype(int) % 8\n",
    "    \n",
    "    return symbols\n",
    "\n",
    "# Apply demodulation to the sampled signal\n",
    "demodulated_symbols = demodulate_8psk(sampled_signal, phase_offset=np.pi/8)\n",
    "\n",
    "print(f\"Demodulated {len(demodulated_symbols)} symbols\")\n",
    "print(f\"Symbol range: {demodulated_symbols.min()} to {demodulated_symbols.max()}\")\n",
    "print(f\"First 20 symbols: {demodulated_symbols[:20]}\")\n",
    "\n",
    "# Count symbol occurrences\n",
    "symbol_counts = np.bincount(demodulated_symbols, minlength=8)\n",
    "print(f\"\\nSymbol distribution:\")\n",
    "for i, count in enumerate(symbol_counts):\n",
    "    print(f\"Symbol {i}: {count} occurrences ({count/len(demodulated_symbols)*100:.1f}%)\")\n",
    "\n",
    "# Plot symbol distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(8), symbol_counts)\n",
    "plt.xlabel('Symbol')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Symbol Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(range(8))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(min(100, len(demodulated_symbols))), demodulated_symbols[:100], 'o-', markersize=4)\n",
    "plt.xlabel('Symbol Index')\n",
    "plt.ylabel('Demodulated Symbol')\n",
    "plt.title('First 100 Demodulated Symbols')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yticks(range(8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate phase angles for verification\n",
    "phases_deg = np.angle(sampled_signal, deg=True)\n",
    "print(f\"\\nPhase statistics:\")\n",
    "print(f\"Phase range: {phases_deg.min():.1f}° to {phases_deg.max():.1f}°\")\n",
    "print(f\"Phase mean: {phases_deg.mean():.1f}°\")\n",
    "print(f\"Phase std: {phases_deg.std():.1f}°\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
