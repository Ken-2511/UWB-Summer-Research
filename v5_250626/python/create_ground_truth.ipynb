{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell is used to create the ground truth for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Process original PPM data and generate ground truth\n",
    "Read raw data following csv_conversion.ipynb method, but downsample to 256G instead of 32G\n",
    "No noise added, used for generating ground truth\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# Set global font\n",
    "matplotlib.rcParams['font.family'] = 'Arial'\n",
    "\n",
    "# Define input filename (original PPM data)\n",
    "input_filename = \"../csv/8PPM_500Mbps.csv\"\n",
    "\n",
    "# Define target resampling frequency as 256G\n",
    "target_frequency_hz = 256 * 1e9  # 256 GHz\n",
    "resampling_interval_s = 1 / target_frequency_hz\n",
    "\n",
    "print(f\"Target resampling frequency: {target_frequency_hz/1e9} GHz\")\n",
    "print(f\"Resampling interval: {resampling_interval_s:.2e} s\")\n",
    "\n",
    "try:\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(input_filename, header=0)\n",
    "    \n",
    "    # Rename columns for easier processing\n",
    "    df.columns = ['original_time', 'original_data']\n",
    "    \n",
    "    # Convert to numeric type\n",
    "    df['original_time'] = pd.to_numeric(df['original_time'], errors='coerce')\n",
    "    df['original_data'] = pd.to_numeric(df['original_data'], errors='coerce')\n",
    "    \n",
    "    # Remove rows containing NaN\n",
    "    df.dropna(subset=['original_time', 'original_data'], inplace=True)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(f\"Error: No valid numeric data found in file {input_filename}\")\n",
    "    else:\n",
    "        # Adjust time axis to start from 0\n",
    "        min_time = df['original_time'].min()\n",
    "        df['time'] = df['original_time'] - min_time\n",
    "        \n",
    "        # Prepare for resampling\n",
    "        start_resample_time = 0\n",
    "        end_resample_time = df['time'].max()\n",
    "        \n",
    "        new_time_axis = np.arange(start_resample_time, end_resample_time, resampling_interval_s)\n",
    "        \n",
    "        # Perform linear interpolation\n",
    "        df_sorted = df.sort_values(by='time')\n",
    "        resampled_data_values = np.interp(new_time_axis, df_sorted['time'], df_sorted['original_data'])\n",
    "        \n",
    "        # Create resampled DataFrame\n",
    "        df_256ghz = pd.DataFrame({'time': new_time_axis, 'data': resampled_data_values})\n",
    "        \n",
    "        print(f\"Processing complete\")\n",
    "        print(f\"Original data shape: {df.shape}\")\n",
    "        print(f\"Data shape after 256G resampling: {df_256ghz.shape}\")\n",
    "        print(f\"Data time range: 0 to {df_256ghz['time'].max():.2e} s\")\n",
    "        print(\"First few rows of data:\")\n",
    "        print(df_256ghz.head())\n",
    "        \n",
    "        # Visualize first 10ns of data\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        time_limit = 1e-8\n",
    "        mask = df_256ghz['time'] <= time_limit\n",
    "        plt.plot(df_256ghz.loc[mask, 'time'], df_256ghz.loc[mask, 'data'])\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Data')\n",
    "        plt.title('PPM Data after 256G Resampling (First 10ns)')\n",
    "        plt.grid(True)\n",
    "        plt.xlim(0, time_limit)\n",
    "        plt.show()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File {input_filename} not found\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred during processing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time alignment processing (following the method in csv_conversion.ipynb)\n",
    "# Remove the initial data so that the symbol aligns to the correct time\n",
    "\n",
    "# Calculate the square of the data for power analysis\n",
    "df_256ghz['Data_Squared'] = df_256ghz['data'] ** 2\n",
    "\n",
    "# Define folding period (assume 500 Mbps symbol rate -> 2 ns period)\n",
    "period = 2e-9  # 2 ns\n",
    "\n",
    "# Use squared data for folding analysis\n",
    "data_to_fold = df_256ghz['Data_Squared'].dropna()\n",
    "time_to_fold = df_256ghz.loc[data_to_fold.index, 'time']\n",
    "\n",
    "# Calculate time modulo period\n",
    "folded_time = time_to_fold % period\n",
    "\n",
    "# Determine time resolution\n",
    "time_resolution = time_to_fold.diff().mean()\n",
    "if pd.isna(time_resolution):\n",
    "    time_resolution = (df_256ghz['time'].iloc[1] - df_256ghz['time'].iloc[0]) if len(df_256ghz['time']) > 1 else 1e-12\n",
    "\n",
    "# Create bins in the range 0-2ns\n",
    "num_bins = max(1, int(period / time_resolution))\n",
    "bins = np.linspace(0, period, num_bins + 1)\n",
    "\n",
    "# Create DataFrame for folding analysis\n",
    "fold_df = pd.DataFrame({'time': folded_time, 'data': data_to_fold})\n",
    "\n",
    "# Assign folded time to bins\n",
    "fold_df['time_bin'] = pd.cut(fold_df['time'], bins=bins, labels=False, include_lowest=True)\n",
    "\n",
    "# Group by bin and sum data\n",
    "summed_data = fold_df.groupby('time_bin')['data'].sum()\n",
    "\n",
    "# Create bin center time axis\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "# Reindex summed data to match bins, fill missing bins with 0\n",
    "summed_data = summed_data.reindex(range(len(bin_centers)), fill_value=0)\n",
    "\n",
    "# Normalize summed data to 0-1 range\n",
    "if summed_data.max() > summed_data.min():\n",
    "    summed_data = (summed_data - summed_data.min()) / (summed_data.max() - summed_data.min())\n",
    "\n",
    "# Visualize folding result\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(bin_centers, summed_data)\n",
    "plt.xlabel('Time (s) within 2ns period')\n",
    "plt.ylabel('Summed Data (Normalized)')\n",
    "plt.title('Folding and Summing Data within 2ns Period')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find the midpoint of intervals above the threshold\n",
    "threshold = 0.2\n",
    "\n",
    "# Find positions where data is above the threshold\n",
    "above_threshold = summed_data > threshold\n",
    "\n",
    "if above_threshold.any():\n",
    "    # Get the time values of these points\n",
    "    time_above_threshold = bin_centers[above_threshold]\n",
    "    \n",
    "    # Calculate the circular mean of the time points to handle wrap-around\n",
    "    # Convert time to angle (radians)\n",
    "    angles = (time_above_threshold / period) * 2 * np.pi\n",
    "    \n",
    "    # Calculate mean sine and cosine of the angles\n",
    "    mean_sin = np.mean(np.sin(angles))\n",
    "    mean_cos = np.mean(np.cos(angles))\n",
    "    \n",
    "    # Calculate mean angle from mean sine and cosine\n",
    "    mean_angle = np.arctan2(mean_sin, mean_cos)\n",
    "    \n",
    "    # Convert mean angle back to time\n",
    "    midpoint_time = (mean_angle / (2 * np.pi)) * period\n",
    "    \n",
    "    # Adjust midpoint to [0, period] range\n",
    "    if midpoint_time < 0:\n",
    "        midpoint_time += period\n",
    "    \n",
    "    print(f\"Midpoint position of interval where data > {threshold}: {midpoint_time:.4e} s\")\n",
    "    print(f\"Need to delay signal by {period - midpoint_time:.4e} s to align to end of period\")\n",
    "    print(f\"Corresponds to {int((period - midpoint_time) * 256e9)} samples at 256 GHz sampling rate\")\n",
    "    \n",
    "    # Visualize midpoint\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(bin_centers, summed_data, label='Summed Data')\n",
    "    plt.axhline(y=threshold, color='r', linestyle='--', label=f'Threshold ({threshold})')\n",
    "    plt.axvline(x=midpoint_time, color='g', linestyle='-', label=f'Midpoint ({midpoint_time:.2e} s)')\n",
    "    plt.xlabel('Time (s) within 2ns period')\n",
    "    plt.ylabel('Summed Data (Normalized)')\n",
    "    plt.title('Folding and Summing Data within 2ns Period with Midpoint Marked')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Remove initial data for time alignment\n",
    "    # Calculate time threshold\n",
    "    time_threshold = midpoint_time + 1e-9  # Add 1ns to midpoint\n",
    "    \n",
    "    # Find indices to remove\n",
    "    indices_to_remove = df_256ghz['time'] < time_threshold\n",
    "    num_points_to_remove = indices_to_remove.sum()\n",
    "    \n",
    "    print(f\"Time threshold: {time_threshold:.4e} s\")\n",
    "    print(f\"Number of data points to remove: {num_points_to_remove}\")\n",
    "    \n",
    "    # Create trimmed dataframe\n",
    "    df_256ghz_aligned = df_256ghz[~indices_to_remove].copy()\n",
    "    \n",
    "    # Reset time axis to start from 0\n",
    "    df_256ghz_aligned['time'] = df_256ghz_aligned['time'] - df_256ghz_aligned['time'].min()\n",
    "    \n",
    "    # Reset index\n",
    "    df_256ghz_aligned.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(f\"Original data shape: {df_256ghz.shape}\")\n",
    "    print(f\"Aligned data shape: {df_256ghz_aligned.shape}\")\n",
    "    print(f\"Number of data points removed: {df_256ghz.shape[0] - df_256ghz_aligned.shape[0]}\")\n",
    "    print(\"First few rows of aligned data:\")\n",
    "    print(df_256ghz_aligned.head())\n",
    "    \n",
    "    # Visualize aligned data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    time_limit = 3e-8\n",
    "    mask = df_256ghz_aligned['time'] <= time_limit\n",
    "    plt.plot(df_256ghz_aligned.loc[mask, 'time'], df_256ghz_aligned.loc[mask, 'data'])\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Data')\n",
    "    plt.title('Aligned Data (First 30ns)')\n",
    "    plt.grid(True)\n",
    "    plt.xlim(0, time_limit)\n",
    "    # Set x-axis ticks every 2ns\n",
    "    plt.xticks(np.arange(0, time_limit + 2e-9, 2e-9))\n",
    "    plt.show()\n",
    "    \n",
    "    # Update main DataFrame to use aligned data\n",
    "    df_256ghz = df_256ghz_aligned.copy()\n",
    "    print(\"\\nTime alignment complete, subsequent processing will use aligned data\")\n",
    "    \n",
    "else:\n",
    "    print(f\"No data points found above threshold {threshold}, skipping time alignment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPM data decoding processing (refer to analyze.ipynb)\n",
    "from scipy.signal import butter, filtfilt\n",
    "from scipy import signal\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Calculate the square of the data (power)\n",
    "df_256ghz['Data_Squared'] = df_256ghz['data'] ** 2\n",
    "\n",
    "# Visualize squared data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_256ghz['time'], df_256ghz['Data_Squared'])\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Data Squared')\n",
    "plt.title('Data Squared vs Time')\n",
    "plt.grid(True)\n",
    "plt.xlim(0, 2e-8)\n",
    "plt.show()\n",
    "\n",
    "# Butterworth low-pass filter parameters\n",
    "native_sampling_rate = 256e9  # 256 GHz sampling rate\n",
    "cutoff = 4e9  # 4 GHz cutoff frequency\n",
    "N = 4  # Filter order\n",
    "nyq = native_sampling_rate / 2\n",
    "cutoff_norm = cutoff / nyq\n",
    "\n",
    "# Design Butterworth filter\n",
    "b, a = butter(N, cutoff_norm, btype='low')\n",
    "\n",
    "# Apply zero-phase filtering\n",
    "df_256ghz['Data_MA'] = filtfilt(b, a, df_256ghz['Data_Squared'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_256ghz['time'], df_256ghz['Data_Squared'], label='Data Squared', alpha=0.7)\n",
    "plt.plot(df_256ghz['time'], df_256ghz['Data_MA'], label=f'Butterworth Low-pass Filter (N={N})', linewidth=2)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Butterworth Low-pass Filter Effect')\n",
    "plt.legend()\n",
    "plt.xlim(0, 2e-8)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Filtering complete\")\n",
    "print(f\"Filter parameters: Cutoff frequency={cutoff/1e9}G Hz, Order={N}\")\n",
    "print(f\"Filtered data range: {df_256ghz['Data_MA'].min():.6f} to {df_256ghz['Data_MA'].max():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulse detection and edge identification\n",
    "# Set threshold\n",
    "threshold = 0.002\n",
    "symbol_rate = 500e6  # 500 MHz\n",
    "\n",
    "# Get filtered data\n",
    "filtered_data = df_256ghz[\"Data_MA\"].values\n",
    "\n",
    "# Calculate rising and falling edges\n",
    "above = filtered_data > threshold\n",
    "rising_edges = np.where(np.diff(above.astype(int)) == 1)[0] + 1  # Rising edge\n",
    "falling_edges = np.where(np.diff(above.astype(int)) == -1)[0] + 1  # Falling edge\n",
    "\n",
    "# Ensure each rising edge is followed by a falling edge\n",
    "if falling_edges.size > 0 and rising_edges.size > 0:\n",
    "    if falling_edges[0] < rising_edges[0]:\n",
    "        falling_edges = falling_edges[1:]\n",
    "    if rising_edges.shape[0] > falling_edges.shape[0]:\n",
    "        rising_edges = rising_edges[:-1]\n",
    "\n",
    "    # Calculate the midpoint of each rising/falling edge pair\n",
    "    mid_indices = ((rising_edges + falling_edges) / 2).astype(int)\n",
    "    mid_times = df_256ghz.loc[mid_indices, 'time'].values\n",
    "\n",
    "    print(f\"Detected {len(mid_times)} pulse midpoints\")\n",
    "    print(f\"Midpoint times: {mid_times}\")\n",
    "else:\n",
    "    mid_indices = np.array([])\n",
    "    mid_times = np.array([])\n",
    "    print(\"No valid rising/falling edge pairs detected\")\n",
    "\n",
    "# Visualize edge detection results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_256ghz['time'], df_256ghz['Data_MA'], label='Filtered Data (Data_MA)')\n",
    "if rising_edges.size > 0:\n",
    "    plt.plot(df_256ghz.loc[rising_edges, 'time'], filtered_data[rising_edges], 'g^', label='Rising Edge')\n",
    "if falling_edges.size > 0:\n",
    "    plt.plot(df_256ghz.loc[falling_edges, 'time'], filtered_data[falling_edges], 'rv', label='Falling Edge')\n",
    "if mid_indices.size > 0:\n",
    "    plt.plot(df_256ghz.loc[mid_indices, 'time'], filtered_data[mid_indices], 'ko', label='Midpoint')\n",
    "plt.axhline(y=threshold, color='r', linestyle='--', label=f'Threshold ({threshold})')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Filtered Data')\n",
    "plt.title('Rising/Falling Edge and Midpoint Detection')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.xlim(0, 2e-8)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Detection parameters: Threshold={threshold}, Symbol rate={symbol_rate/1e6}M Hz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means clustering analysis for PPM symbol classification\n",
    "if 'mid_indices' in locals() and 'mid_times' in locals() and len(mid_times) > 0:\n",
    "    # Extract list of midpoint positions\n",
    "    mid_indices_list = mid_indices.tolist()\n",
    "    mid_positions = mid_times.tolist()\n",
    "    \n",
    "    print(\"Midpoint analysis result:\")\n",
    "    print(f\"Number of midpoints found: {len(mid_positions)}\")\n",
    "    print(f\"Midpoint indices: {mid_indices_list[:10]}...\")  # Show only first 10\n",
    "    print(f\"Midpoint time positions (seconds): {mid_positions[:10]}...\")  # Show only first 10\n",
    "    \n",
    "    # Convert to nanoseconds for display\n",
    "    mid_positions_ns = [pos * 1e9 for pos in mid_positions]\n",
    "    print(f\"Midpoint time positions (ns): {mid_positions_ns[:10]}...\")\n",
    "    \n",
    "    # Calculate positions relative to the first midpoint\n",
    "    if len(mid_positions) > 1:\n",
    "        relative_positions = [(pos - mid_positions[0]) * 1e9 for pos in mid_positions]\n",
    "        print(f\"Relative positions to first midpoint (ns): {relative_positions[:10]}...\")\n",
    "        \n",
    "        # Calculate intervals between consecutive midpoints\n",
    "        intervals_ns = [pos * 1e9 for pos in np.diff(mid_positions)]\n",
    "        print(f\"Intervals between consecutive midpoints (ns): {intervals_ns[:10]}...\")\n",
    "        \n",
    "    # Use K-means clustering to analyze midpoint positions (modulo 2ns period)\n",
    "    if len(mid_positions_ns) > 8:  # Need at least 8 points for 8 clusters\n",
    "        mid_positions_mod = np.array([pos % 2 for pos in mid_positions_ns]).reshape(-1, 1)\n",
    "        \n",
    "        # Apply K-means, 8 clusters\n",
    "        kmeans = KMeans(n_clusters=8, random_state=42)\n",
    "        cluster_labels_original = kmeans.fit_predict(mid_positions_mod)\n",
    "        cluster_centers = kmeans.cluster_centers_.flatten()\n",
    "        \n",
    "        # Create mapping to sort clusters by time position\n",
    "        center_time_pairs = [(i, center) for i, center in enumerate(cluster_centers)]\n",
    "        center_time_pairs.sort(key=lambda x: x[1])  # Sort by time position\n",
    "        \n",
    "        # Create mapping from original cluster ID to time-sorted cluster ID\n",
    "        old_to_new_mapping = {}\n",
    "        for new_id, (old_id, _) in enumerate(center_time_pairs):\n",
    "            old_to_new_mapping[old_id] = new_id\n",
    "        \n",
    "        # Apply mapping to cluster labels\n",
    "        cluster_labels = np.array([old_to_new_mapping[label] for label in cluster_labels_original])\n",
    "        \n",
    "        # Sort cluster centers by time position\n",
    "        sorted_centers = [pair[1] for pair in center_time_pairs]\n",
    "        \n",
    "        print(\"\\n=== K-means Clustering Result (8 clusters) - Sorted by Time ===\")\n",
    "        for i, center in enumerate(sorted_centers):\n",
    "            cluster_size = np.sum(cluster_labels == i)\n",
    "            print(f\"Cluster {i}: Time = {center:.4f} ns (mod 2), Points = {cluster_size}\")\n",
    "        \n",
    "        # Convert cluster centers to picoseconds and display as list\n",
    "        cluster_centers_ps = [int(round(center * 1000)) for center in sorted_centers]\n",
    "        print(f\"\\nCluster centers (ps, sorted by time): {cluster_centers_ps}\")\n",
    "\n",
    "        # Print nanosecond list\n",
    "        print(f\"\\nCluster centers (ns, sorted by time): [\", end=\"\")\n",
    "        for i in range(len(sorted_centers)):\n",
    "            print(f\"{sorted_centers[i]:.4f}\", end=\" \" if i < len(sorted_centers) - 1 else \"\")\n",
    "        print(\"]\")\n",
    "\n",
    "        # Visualize clustering result\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f']\n",
    "\n",
    "        # Plot cluster centers (now sorted by time)\n",
    "        plt.scatter(sorted_centers, range(8), color='red', marker='x', s=150, \n",
    "                   linewidths=3, label='Cluster Center')\n",
    "        \n",
    "        # Plot cluster points\n",
    "        for i in range(8):\n",
    "            cluster_points = mid_positions_mod[cluster_labels == i]\n",
    "            plt.scatter(cluster_points, np.ones(len(cluster_points)) * i, \n",
    "                       color=colors[i], alpha=0.7, s=30, label=f'Cluster {i}')\n",
    "        \n",
    "        plt.xlabel('Midpoint Position (ns, mod 2)')\n",
    "        plt.ylabel('Cluster ID (Sorted by Time)')\n",
    "        plt.title('K-means Clustering of Midpoint Positions (8 Clusters, Sorted by Time)')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nClustering analysis complete, variables 'cluster_labels' and 'sorted_centers' created\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Not enough midpoints for K-means clustering (found {len(mid_positions_ns)}, need > 8)\")\n",
    "        cluster_labels = None\n",
    "        \n",
    "else:\n",
    "    print(\"Please run the midpoint detection cell first to generate midpoint data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symbol classification and Ground Truth generation\n",
    "# Symbol rate: 500MHz, each symbol period is 2ns, aligned to [0, 2, 4, 6...] ns\n",
    "\n",
    "symbol_rate = 500e6  # 500 MHz\n",
    "symbol_period_ns = 1 / symbol_rate * 1e9  # 2 ns\n",
    "\n",
    "# Get results from previous clustering analysis\n",
    "if 'cluster_labels' in locals() and 'mid_positions_ns' in locals() and cluster_labels is not None:\n",
    "    # Create symbol classification data\n",
    "    symbol_data = []\n",
    "    \n",
    "    # Classify each detected pulse midpoint\n",
    "    for i, (mid_pos_ns, cluster_id) in enumerate(zip(mid_positions_ns, cluster_labels)):\n",
    "        # Calculate the corresponding symbol time slot (2ns period)\n",
    "        symbol_slot = int(round(mid_pos_ns / symbol_period_ns))\n",
    "        \n",
    "        # Use cluster ID as data value (0-7, representing 8 different symbols)\n",
    "        data_value = cluster_id\n",
    "        \n",
    "        symbol_data.append({\n",
    "            'index': i,\n",
    "            'data': data_value\n",
    "        })\n",
    "    \n",
    "    # Create new DataFrame with only index and data columns\n",
    "    ground_truth_df = pd.DataFrame(symbol_data)\n",
    "    \n",
    "    print(\"=== Ground Truth Symbol Classification Result ===\")\n",
    "    print(f\"Total number of classified symbols: {len(ground_truth_df)}\")\n",
    "    print(f\"Symbol period: {symbol_period_ns:.1f} ns\")\n",
    "    print(\"\\nFirst 20 symbol classification results:\")\n",
    "    print(ground_truth_df.head(20))\n",
    "    \n",
    "    print(\"\\nStatistics for each symbol type:\")\n",
    "    data_counts = ground_truth_df['data'].value_counts().sort_index()\n",
    "    for data_val, count in data_counts.items():\n",
    "        print(f\"Symbol type {data_val}: {count} occurrences\")\n",
    "    \n",
    "    # Visualize symbol classification\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Subplot 1: Symbol sequence\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(ground_truth_df['index'], ground_truth_df['data'], 'o-', markersize=3, linewidth=0.5)\n",
    "    plt.xlabel('Symbol Index')\n",
    "    plt.ylabel('Symbol Type')\n",
    "    plt.title('Ground Truth Symbol Sequence')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim(0, min(100, len(ground_truth_df)))  # Show first 100 symbols\n",
    "    \n",
    "    # Subplot 2: Symbol type distribution\n",
    "    plt.subplot(2, 2, 2)\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f']\n",
    "    data_counts.plot(kind='bar', color=colors[:len(data_counts)])\n",
    "    plt.xlabel('Symbol Type')\n",
    "    plt.ylabel('Occurrences')\n",
    "    plt.title('Symbol Type Distribution')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 3: Histogram of symbol sequence\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.hist(ground_truth_df['data'], bins=8, alpha=0.7, edgecolor='black', color='skyblue')\n",
    "    plt.xlabel('Symbol Type')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Symbol Type Frequency Distribution')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 4: Symbol transition matrix (simplified)\n",
    "    plt.subplot(2, 2, 4)\n",
    "    if len(ground_truth_df) > 1:\n",
    "        transitions = np.zeros((8, 8))\n",
    "        for i in range(len(ground_truth_df) - 1):\n",
    "            from_symbol = ground_truth_df.iloc[i]['data']\n",
    "            to_symbol = ground_truth_df.iloc[i + 1]['data']\n",
    "            transitions[from_symbol, to_symbol] += 1\n",
    "        \n",
    "        plt.imshow(transitions, cmap='Blues', interpolation='nearest')\n",
    "        plt.xlabel('To Symbol')\n",
    "        plt.ylabel('From Symbol')\n",
    "        plt.title('Symbol Transition Matrix')\n",
    "        plt.colorbar()\n",
    "        for i in range(8):\n",
    "            for j in range(8):\n",
    "                plt.text(j, i, str(int(transitions[i, j])), ha='center', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save Ground Truth DataFrame as CSV file\n",
    "    output_csv_path = '../csv/ground_truth_ppm.csv'\n",
    "    ground_truth_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"\\nGround Truth data saved to: {output_csv_path}\")\n",
    "    print(f\"Saved DataFrame shape: {ground_truth_df.shape}\")\n",
    "    print(f\"Column names: {list(ground_truth_df.columns)}\")\n",
    "    \n",
    "    # Show first few rows of saved file for verification\n",
    "    print(\"\\nSaved file verification (first 10 rows):\")\n",
    "    saved_df = pd.read_csv(output_csv_path)\n",
    "    print(saved_df.head(10))\n",
    "    \n",
    "else:\n",
    "    print(\"Error: Please run the previous clustering analysis cell to generate clustering results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Ground Truth data repeats with a period of 127\n",
    "# Verify periodic characteristics of the PPM sequence\n",
    "\n",
    "if 'ground_truth_df' in locals() and len(ground_truth_df) > 0:\n",
    "    \n",
    "    # Get data sequence\n",
    "    data_sequence = ground_truth_df['data'].values\n",
    "    total_length = len(data_sequence)\n",
    "    \n",
    "    print(f\"=== Ground Truth Periodicity Analysis ===\")\n",
    "    print(f\"Total data length: {total_length}\")\n",
    "    print(f\"Check period: 127\")\n",
    "    \n",
    "    # Check if divisible by 127\n",
    "    if total_length >= 127:\n",
    "        complete_periods = total_length // 127\n",
    "        remaining_samples = total_length % 127\n",
    "        \n",
    "        print(f\"Number of complete periods: {complete_periods}\")\n",
    "        print(f\"Number of remaining samples: {remaining_samples}\")\n",
    "        \n",
    "        if complete_periods >= 2:  # Need at least two complete periods to compare\n",
    "            \n",
    "            # Extract the first 127 samples as reference pattern\n",
    "            reference_pattern = data_sequence[:127]\n",
    "            \n",
    "            # Check the match of each complete period with the reference pattern\n",
    "            period_matches = []\n",
    "            period_differences = []\n",
    "            \n",
    "            for period_idx in range(complete_periods):\n",
    "                start_idx = period_idx * 127\n",
    "                end_idx = start_idx + 127\n",
    "                current_period = data_sequence[start_idx:end_idx]\n",
    "                \n",
    "                # Calculate match\n",
    "                matches = np.sum(current_period == reference_pattern)\n",
    "                match_percentage = (matches / 127) * 100\n",
    "                period_matches.append(match_percentage)\n",
    "                \n",
    "                # Calculate difference positions\n",
    "                differences = np.where(current_period != reference_pattern)[0]\n",
    "                period_differences.append(len(differences))\n",
    "                \n",
    "                print(f\"Period {period_idx + 1}: Match {match_percentage:.1f}% ({matches}/127), Differences {len(differences)}\")\n",
    "            \n",
    "            # Overall statistics\n",
    "            avg_match = np.mean(period_matches)\n",
    "            min_match = np.min(period_matches)\n",
    "            max_match = np.max(period_matches)\n",
    "            \n",
    "            print(f\"\\n=== Periodicity Statistics ===\")\n",
    "            print(f\"Average match: {avg_match:.1f}%\")\n",
    "            print(f\"Minimum match: {min_match:.1f}%\")\n",
    "            print(f\"Maximum match: {max_match:.1f}%\")\n",
    "            \n",
    "            # Determine if periodic\n",
    "            is_periodic = avg_match > 99.0  # Consider periodic if match > 99%\n",
    "            print(f\"Is 127-periodic: {'Yes' if is_periodic else 'No'}\")\n",
    "            \n",
    "            # Visualization analysis\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "            \n",
    "            # Subplot 1: Comparison of first few periods\n",
    "            ax1 = axes[0, 0]\n",
    "            periods_to_show = min(4, complete_periods)\n",
    "            for i in range(periods_to_show):\n",
    "                start_idx = i * 127\n",
    "                end_idx = start_idx + 127\n",
    "                period_data = data_sequence[start_idx:end_idx]\n",
    "                ax1.plot(range(127), period_data, 'o-', markersize=2, \n",
    "                        label=f'Period {i+1}', alpha=0.7)\n",
    "            \n",
    "            ax1.set_xlabel('Position within period')\n",
    "            ax1.set_ylabel('Symbol Value')\n",
    "            ax1.set_title(f'Comparison of First {periods_to_show} Periods')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Subplot 2: Match percentage bar chart\n",
    "            ax2 = axes[0, 1]\n",
    "            bars = ax2.bar(range(1, complete_periods + 1), period_matches, \n",
    "                          color='skyblue', alpha=0.7, edgecolor='black')\n",
    "            ax2.set_xlabel('Period Number')\n",
    "            ax2.set_ylabel('Match (%)')\n",
    "            ax2.set_title('Match Percentage of Each Period with Reference Pattern')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            ax2.set_ylim(0, 105)\n",
    "            \n",
    "            # Annotate values on bar chart\n",
    "            for i, bar in enumerate(bars):\n",
    "                height = bar.get_height()\n",
    "                ax2.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                        f'{height:.1f}%', ha='center', va='bottom', fontsize=8)\n",
    "            \n",
    "            # Subplot 3: Difference position heatmap (if there are differences)\n",
    "            ax3 = axes[1, 0]\n",
    "            if complete_periods >= 2:\n",
    "                difference_matrix = np.zeros((complete_periods, 127))\n",
    "                for period_idx in range(complete_periods):\n",
    "                    start_idx = period_idx * 127\n",
    "                    end_idx = start_idx + 127\n",
    "                    current_period = data_sequence[start_idx:end_idx]\n",
    "                    differences = (current_period != reference_pattern).astype(int)\n",
    "                    difference_matrix[period_idx, :] = differences\n",
    "                \n",
    "                im = ax3.imshow(difference_matrix, cmap='Reds', aspect='auto', interpolation='nearest')\n",
    "                ax3.set_xlabel('Position within period')\n",
    "                ax3.set_ylabel('Period Number')\n",
    "                ax3.set_title('Difference Position Heatmap (Red=Difference)')\n",
    "                plt.colorbar(im, ax=ax3)\n",
    "            \n",
    "            # Subplot 4: Reference pattern visualization\n",
    "            ax4 = axes[1, 1]\n",
    "            ax4.plot(range(127), reference_pattern, 'o-', markersize=3, linewidth=1.5, color='darkblue')\n",
    "            ax4.set_xlabel('Position')\n",
    "            ax4.set_ylabel('Symbol Value')\n",
    "            ax4.set_title('Reference Pattern (First 127 Symbols)')\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "            ax4.set_ylim(-0.5, 7.5)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Detailed periodicity analysis\n",
    "            if not is_periodic and complete_periods >= 2:\n",
    "                print(f\"\\n=== Detailed Non-periodicity Analysis ===\")\n",
    "                \n",
    "                # Find most common difference positions\n",
    "                all_diff_positions = []\n",
    "                for period_idx in range(1, complete_periods):  # Skip first period (reference)\n",
    "                    start_idx = period_idx * 127\n",
    "                    end_idx = start_idx + 127\n",
    "                    current_period = data_sequence[start_idx:end_idx]\n",
    "                    diff_positions = np.where(current_period != reference_pattern)[0]\n",
    "                    all_diff_positions.extend(diff_positions)\n",
    "                \n",
    "                if all_diff_positions:\n",
    "                    from collections import Counter\n",
    "                    diff_counter = Counter(all_diff_positions)\n",
    "                    most_common_diffs = diff_counter.most_common(10)\n",
    "                    \n",
    "                    print(\"Most common difference positions:\")\n",
    "                    for pos, count in most_common_diffs:\n",
    "                        print(f\"  Position {pos}: {count} differences\")\n",
    "            \n",
    "            # Save periodicity analysis result\n",
    "            period_analysis = {\n",
    "                'total_length': total_length,\n",
    "                'complete_periods': complete_periods,\n",
    "                'remaining_samples': remaining_samples,\n",
    "                'average_match_percentage': avg_match,\n",
    "                'is_periodic': is_periodic,\n",
    "                'period_matches': period_matches\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nPeriodicity analysis complete, result saved in variable 'period_analysis'\")\n",
    "            \n",
    "        else:\n",
    "            print(\"Data length insufficient, need at least 2 complete 127 periods for comparison\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"Data length {total_length} is less than 127, cannot perform periodicity check\")\n",
    "        \n",
    "    # Extra: Check other possible period lengths\n",
    "    print(f\"\\n=== Check Other Period Lengths ===\")\n",
    "    possible_periods = [31, 63, 127, 255, 511]  # Common PRBS period lengths\n",
    "    \n",
    "    for period_len in possible_periods:\n",
    "        if total_length >= period_len * 2:  # Need at least two periods\n",
    "            first_period = data_sequence[:period_len]\n",
    "            second_period = data_sequence[period_len:2*period_len]\n",
    "            matches = np.sum(first_period == second_period)\n",
    "            match_rate = (matches / period_len) * 100\n",
    "            print(f\"Period length {period_len}: Match {match_rate:.1f}%\")\n",
    "        else:\n",
    "            print(f\"Period length {period_len}: Not enough data\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: ground_truth_df data not found, please run the previous symbol classification cell first\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
